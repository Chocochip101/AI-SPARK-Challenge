{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Uwn7ixzMQx1"
      },
      "source": [
        "# Feature Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLPbMeezy9kS"
      },
      "source": [
        "한글 깨짐 방지 압출 풀기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNTrbMZveTp1",
        "outputId": "44c9a015-db6f-4b8c-c00a-5e16c3252472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/비디오 요약 영상/Training/SUMVID_LONG_TRAIN_01.zip\n",
            "  inflating: 1_미 대선 결과와 한반도 정세, 국내 정치권 동향, 검찰 특활비, 대권구도, 시도통합   LIVE 시사본색 2020년 11월 15일.mp4  \n",
            "  inflating: 2_4월 보궐선거 공천 논란과 지역 현안   잃어버린 고대왕국 마한, 실체 드러내나   LIVE 시사본색 2020년 11월 08일.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190705_005921.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190712_005856.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190726_005600.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190802_005537.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190816_005908.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190823_005835.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190830_010049.mp4  \n",
            "  inflating: 3_K트롯서바이벌골든마이크_20190906_005825.mp4  \n"
          ]
        }
      ],
      "source": [
        "!unzip -O cp949 \"/content/drive/MyDrive/비디오 요약 영상/Training/SUMVID_LONG_TRAIN_01.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbAdgxDPzV3M",
        "outputId": "7bb29ada-9738-43a8-e49a-fa5e6a03a101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSVA\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/MSVA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTTTXZMIMarJ"
      },
      "source": [
        "Extract npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8Ocm2Si1E2E",
        "outputId": "93c0354f-05dd-48f6-90aa-24270c4aa383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py==0.12.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: cached-property==1.5.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: certifi==2020.12.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.0.0)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: h5py==3.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: imageio==2.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.9.0)\n",
            "Requirement already satisfied: imageio-ffmpeg==0.4.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.4.3)\n",
            "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.0.3)\n",
            "Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.19.5)\n",
            "Requirement already satisfied: opencv-python==4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (4.5.1.48)\n",
            "Requirement already satisfied: ortools==8.2.8710 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (8.2.8710)\n",
            "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (1.1.5)\n",
            "Requirement already satisfied: Pillow==8.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (8.1.2)\n",
            "Requirement already satisfied: proglog==0.1.9 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (0.1.9)\n",
            "Requirement already satisfied: protobuf==3.15.6 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (3.15.6)\n",
            "Requirement already satisfied: python-dateutil==2.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (2.8.1)\n",
            "Requirement already satisfied: pytz==2021.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (2021.1)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (2.25.1)\n",
            "Requirement already satisfied: scipy==1.5.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (1.5.4)\n",
            "Requirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (1.15.0)\n",
            "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 23)) (1.8.0)\n",
            "Requirement already satisfied: torchvision==0.9.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 24)) (0.9.0)\n",
            "Requirement already satisfied: tqdm==4.59.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 25)) (4.59.0)\n",
            "Requirement already satisfied: typing-extensions==3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 26)) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3==1.26.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 27)) (1.26.4)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 29)) (0.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from ffmpeg-python->-r requirements.txt (line 29)) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaGBh9nr1GJL",
        "outputId": "591a2f80-53d9-43f6-ca71-beff8d28b883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 2D-ResNet-152 ...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100% 230M/230M [00:00<00:00, 352MB/s]\n",
            "loaded\n",
            "Decoding video: /content/drive/MyDrive/MSVA/samplevideo/JungleVideo.mp4\n",
            "tcmalloc: large alloc 1131880448 bytes == 0x5574c21d8000 @  0x7ffb5ff861e7 0x7ffb0db8c46e 0x7ffb0dbdcc7b 0x7ffb0dbdcd97 0x7ffb0dc75887 0x55742fdac160 0x55742fe9dd4d 0x55742fe1fec8 0x55742fdaf83b 0x55742fdf0b29 0x55742fdf0a9c 0x55742fe94419 0x55742fe1be5c 0x55742fe1acdd 0x55742fdad88a 0x55742fe1b8f6 0x55742fe1acdd 0x55742fdad88a 0x55742fe1bb4f 0x55742fcecd14 0x55742fe1cff1 0x55742fdad7aa 0x55742fe1bb4f 0x55742fdad7aa 0x55742fe1bb4f 0x55742fdad7aa 0x55742fe1bb4f 0x55742fdadce9 0x55742fdef239 0x55742fdec184 0x55742fdac902\n",
            "Computing features of video 1/1: /content/drive/MyDrive/MSVA/samplevideo/JungleVideo.mp4\n",
            "tcmalloc: large alloc 1131880448 bytes == 0x5574a9dc8000 @  0x7ffb5ff68b6b 0x7ffb5ff88379 0x7ffb0e49725e 0x7ffb0e4989d2 0x7ffb4937d646 0x7ffb497dfb69 0x7ffb49ce9f0a 0x7ffb49cb5689 0x7ffb49c6bde7 0x7ffb49b100b9 0x7ffb49364545 0x7ffb49366c22 0x7ffb49368f8b 0x7ffb4936a4ed 0x7ffb494f18b4 0x7ffb49cea59f 0x7ffb49a99fd6 0x7ffb49aa09ff 0x7ffb4b3937e6 0x7ffb4b3939cf 0x7ffb49eec1e6 0x7ffb49ef16cf 0x7ffb5aec34de 0x7ffb5aec41e6 0x55742fec1904 0x55742fdac3a2 0x55742fdf1960 0x55742fdf30ec 0x55742fd8f643 0x55742fe964c6 0x55742fe1dd4a\n"
          ]
        }
      ],
      "source": [
        "!python extract.py --csv=SampleVideo.csv --type=2d --batch_size=8 --num_decoding_thread=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-EqyASZ4rD0"
      },
      "source": [
        "Convert to h5 file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRVoSqTz4qVO",
        "outputId": "5e47e6f0-b466-4049-ec8c-1c548a714e42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2_4월 보궐선거 공천 논란과 지역 현안   잃어버린 고대왕국 마한, 실체 드러내나   LIVE 시사본색 2020년 11월 08일.npy', '3_K트롯서바이벌골든마이크_20190705_005921.npy', '3_K트롯서바이벌골든마이크_20190712_005856.npy', '3_K트롯서바이벌골든마이크_20190726_005600.npy', '3_K트롯서바이벌골든마이크_20190802_005537.npy', '3_K트롯서바이벌골든마이크_20190816_005908.npy', '3_K트롯서바이벌골든마이크_20190823_005835.npy', '3_K트롯서바이벌골든마이크_20190830_010049.npy', '3_K트롯서바이벌골든마이크_20190906_005825.npy', 'SampleVideo.npy', 'SUMVID_SHORT_TRAIN_01_1.npy', 'JungleVideo.npy']\n"
          ]
        }
      ],
      "source": [
        "!python convert.py --numpy_dir=/NumpyResult --output_dir=/h5Result --ouput_filename=SampleVideo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHmsRfaEMdxD"
      },
      "source": [
        "# MSVA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWC4iEDGs026"
      },
      "source": [
        "h5 File should be at ./datasets/object_feature  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYJBBU0T3I0Q"
      },
      "outputs": [],
      "source": [
        "import shutil \n",
        "\n",
        "filename = 'SampleVideo.h5' \n",
        "src = './h5Result/' \n",
        "dir = './datasets/object_features/' \n",
        "try:\n",
        "  shutil.move(src + filename, dir + filename)\n",
        "except:\n",
        "  print(\"Dir Error!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jINEeiWSvlB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7xw1s2QOuMa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAZXeGGTC39G",
        "outputId": "6de31f90-29fb-4126-d038-e5b52eb9ea60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters:\n",
            "----------------------------------------------------------------------\n",
            "{'verbose': False, 'train': True, 'use_cuda': True, 'cuda_device': 0, 'max_summary_length': 0.15, 'weight_decay': 1e-05, 'lr': [5e-05], 'epochs_max': 300, 'train_batch_size': 5, 'root': '', 'fusion_technique': 'inter', 'method': 'mean', 'sample_technique': 'sub', 'stack': 'v', 'name_anchor': 'inter_add_aperture_250', 'output_dir': './results/', 'base_dir': '', 'apertures': [250], 'local': False, 'combis': [[1, 1, 1, 1]], 'feat_input': {'feature_size': 365, 'L1_out': 365, 'L2_out': 365, 'L3_out': 512, 'pred_out': 1, 'apperture': 250, 'dropout1': 0.5, 'att_dropout1': 0.5, 'feature_size_1_3': 1024, 'feature_size_4': 365}, 'object_features': ['datasets/object_features/eccv16_dataset_summe_google_pool5.h5', 'datasets/object_features/eccv16_dataset_tvsum_google_pool5.h5'], 'kinetic_features': './datasets/kinetic_features/', 'splits': ['splits/tvsum_splits_from_early_work.json', 'splits/summe_splits_from_early_work.json', 'splits/summe_random_non_overlap_splits.json', 'splits/tvsum_random_non_overlap_splits.json', 'splits/summe_ordered_non_overlap_splits.json', 'splits/tvsum_ordered_non_overlap_splits.json']}\n",
            "----------------------------------------------------------------------\n",
            "randomSeed:  12345\n",
            "feat_input <class '__main__.obj'>\n",
            "Setting CUDA device:  0\n",
            "Loading: datasets/object_features/eccv16_dataset_tvsum_google_pool5.h5\n",
            "Loading splits from:  splits/tvsum_splits_from_early_work.json\n",
            "Selecting split:  0\n",
            "[1, 1, 1, 1]\n",
            "Initializing model and optimizer for Feature Combination:  [1, 1, 1, 1]\n",
            "Starting training...\n",
            "Epoch  0  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.1178902866318822\n",
            "   Test F-score avg:  0.50301593749492\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.50301593749492\n",
            "----------------------------------------------------------------------\n",
            "Epoch  1  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0948765005916357\n",
            "   Test F-score avg:  0.570106624193924\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.570106624193924\n",
            "----------------------------------------------------------------------\n",
            "Epoch  2  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.08226183354854584\n",
            "   Test F-score avg:  0.551863031538334\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.570106624193924\n",
            "----------------------------------------------------------------------\n",
            "Epoch  3  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.07449770467355847\n",
            "   Test F-score avg:  0.5551108337074278\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.570106624193924\n",
            "----------------------------------------------------------------------\n",
            "Epoch  4  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.06737972553819419\n",
            "   Test F-score avg:  0.5805759773038321\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  5  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.06402602307498455\n",
            "   Test F-score avg:  0.5786403366023926\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  6  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.061921618226915595\n",
            "   Test F-score avg:  0.5694201058961244\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  7  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.06021149167791009\n",
            "   Test F-score avg:  0.571005426482289\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  8  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05928210150450468\n",
            "   Test F-score avg:  0.5689000283899961\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  9  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0580795188434422\n",
            "   Test F-score avg:  0.5641115385815026\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  10  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05728623759932816\n",
            "   Test F-score avg:  0.5646149034512068\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  11  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0565607488155365\n",
            "   Test F-score avg:  0.572928057162684\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  12  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05608084611594677\n",
            "   Test F-score avg:  0.5774700538475869\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  13  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05534859420731664\n",
            "   Test F-score avg:  0.5742813356588865\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  14  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.054424437880516055\n",
            "   Test F-score avg:  0.5683283797246031\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5805759773038321\n",
            "----------------------------------------------------------------------\n",
            "Epoch  15  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05382117228582502\n",
            "   Test F-score avg:  0.580814730618009\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.580814730618009\n",
            "----------------------------------------------------------------------\n",
            "Epoch  16  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05370292589068413\n",
            "   Test F-score avg:  0.5652449526532636\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.580814730618009\n",
            "----------------------------------------------------------------------\n",
            "Epoch  17  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.05226011811755597\n",
            "   Test F-score avg:  0.5742113703408755\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.580814730618009\n",
            "----------------------------------------------------------------------\n",
            "Epoch  18  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.051136719482019546\n",
            "   Test F-score avg:  0.582933817141374\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.582933817141374\n",
            "----------------------------------------------------------------------\n",
            "Epoch  19  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04991441597230732\n",
            "   Test F-score avg:  0.5698313871351444\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.582933817141374\n",
            "----------------------------------------------------------------------\n",
            "Epoch  20  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04918241961859167\n",
            "   Test F-score avg:  0.5840793909780575\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  21  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04844809072092175\n",
            "   Test F-score avg:  0.5825544817909477\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  22  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04776557600125671\n",
            "   Test F-score avg:  0.5465926309257049\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  23  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04719401062466204\n",
            "   Test F-score avg:  0.5595443083466105\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  24  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04606310916133225\n",
            "   Test F-score avg:  0.554359083899026\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  25  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04559618514031172\n",
            "   Test F-score avg:  0.5373159550503155\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  26  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04512653285637498\n",
            "   Test F-score avg:  0.5499419369385025\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  27  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04390771859325469\n",
            "   Test F-score avg:  0.5403811962528571\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  28  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.044082704652100804\n",
            "   Test F-score avg:  0.5537071428447622\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  29  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04362543229945004\n",
            "   Test F-score avg:  0.5437520687509388\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  30  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.042787287896499035\n",
            "   Test F-score avg:  0.5339096531001241\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  31  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04208429343998432\n",
            "   Test F-score avg:  0.5549656042360152\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  32  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04123351825401187\n",
            "   Test F-score avg:  0.5287539532756205\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  33  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04082078104838729\n",
            "   Test F-score avg:  0.5333416392300518\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  34  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04008418330922723\n",
            "   Test F-score avg:  0.5177669160061343\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  35  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.040982736134901644\n",
            "   Test F-score avg:  0.5246000479035071\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  36  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.04039340866729617\n",
            "   Test F-score avg:  0.5189442940001595\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  37  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.040666552679613235\n",
            "   Test F-score avg:  0.5137542446146647\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  38  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.040179444244131446\n",
            "   Test F-score avg:  0.5244938192336203\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  39  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.039715062454342843\n",
            "   Test F-score avg:  0.5223941965122997\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  40  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03798440499231219\n",
            "   Test F-score avg:  0.5101687449371441\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  41  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03694534795358777\n",
            "   Test F-score avg:  0.5113949349094699\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  42  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03774629021063447\n",
            "   Test F-score avg:  0.5231812393178901\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  43  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03733772188425064\n",
            "   Test F-score avg:  0.5116038295473375\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  44  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03888096669688821\n",
            "   Test F-score avg:  0.5189466843481072\n",
            "   Test F-score min:  0.50301593749492\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  45  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.036562414420768616\n",
            "   Test F-score avg:  0.4926004362803432\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  46  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.036415120400488374\n",
            "   Test F-score avg:  0.5084391415201994\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  47  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0372564684599638\n",
            "   Test F-score avg:  0.526792024828313\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  48  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.035772624192759395\n",
            "   Test F-score avg:  0.5065318084604284\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  49  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03581363018602133\n",
            "   Test F-score avg:  0.5136150234593139\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  50  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.035778543911874294\n",
            "   Test F-score avg:  0.5176057624851339\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  51  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03620004369877279\n",
            "   Test F-score avg:  0.5182534709350454\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  52  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03460615798830986\n",
            "   Test F-score avg:  0.5124701707062022\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  53  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.035848659463226795\n",
            "   Test F-score avg:  0.5178514509826178\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  54  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03536820854060352\n",
            "   Test F-score avg:  0.5151485251841681\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  55  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.036065707076340915\n",
            "   Test F-score avg:  0.5153198241532585\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  56  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03450850765220821\n",
            "   Test F-score avg:  0.510285420616422\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  57  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03335638134740293\n",
            "   Test F-score avg:  0.5162597593157283\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  58  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03273223452270031\n",
            "   Test F-score avg:  0.514530107834265\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  59  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03341370108537376\n",
            "   Test F-score avg:  0.5021432265436061\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  60  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.032169194286689164\n",
            "   Test F-score avg:  0.4948427702566983\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  61  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03326255483552813\n",
            "   Test F-score avg:  0.5185499912855064\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  62  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03239131346344948\n",
            "   Test F-score avg:  0.5125427481545901\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  63  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.033086195960640906\n",
            "   Test F-score avg:  0.525450049866172\n",
            "   Test F-score min:  0.4926004362803432\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  64  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.032633005268871786\n",
            "   Test F-score avg:  0.4766339358265948\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  65  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0351704066619277\n",
            "   Test F-score avg:  0.5163224584301875\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  66  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03330345177091658\n",
            "   Test F-score avg:  0.481452947623539\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  67  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03424664749763906\n",
            "   Test F-score avg:  0.49870682784202075\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  68  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.033013433357700704\n",
            "   Test F-score avg:  0.5208895385679047\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  69  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.032171877287328246\n",
            "   Test F-score avg:  0.4997332673892593\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  70  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03185389013960958\n",
            "   Test F-score avg:  0.5028707291661207\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  71  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03129369979724288\n",
            "   Test F-score avg:  0.5066953344629394\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  72  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03066658661700785\n",
            "   Test F-score avg:  0.5007888998839516\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  73  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.031105300690978766\n",
            "   Test F-score avg:  0.5207310745026245\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  74  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.030902979150414466\n",
            "   Test F-score avg:  0.511547584884912\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  75  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.030443930299952626\n",
            "   Test F-score avg:  0.5002317347987285\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  76  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.029947178764268757\n",
            "   Test F-score avg:  0.5038558620150199\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  77  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.033183907298371194\n",
            "   Test F-score avg:  0.5138285347718943\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  78  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03251222418621182\n",
            "   Test F-score avg:  0.4865714832704094\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  79  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03186148218810558\n",
            "   Test F-score avg:  0.5058765836304324\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  80  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.030369506776332857\n",
            "   Test F-score avg:  0.48434854859750776\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  81  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028914497420191764\n",
            "   Test F-score avg:  0.4949351314072327\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  82  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.029846240114420653\n",
            "   Test F-score avg:  0.5121839352919829\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  83  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.030906181363388897\n",
            "   Test F-score avg:  0.5095552195330385\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  84  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03048984194174409\n",
            "   Test F-score avg:  0.5199553731821446\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  85  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03105204147286713\n",
            "   Test F-score avg:  0.49296735769265076\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  86  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.03004942056722939\n",
            "   Test F-score avg:  0.4900346554374975\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  87  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02913711410947144\n",
            "   Test F-score avg:  0.49854731363497934\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  88  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02832099860534072\n",
            "   Test F-score avg:  0.49561981511526054\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  89  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028317539393901824\n",
            "   Test F-score avg:  0.4885045746479305\n",
            "   Test F-score min:  0.4766339358265948\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  90  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0281394244171679\n",
            "   Test F-score avg:  0.4731421662655954\n",
            "   Test F-score min:  0.4731421662655954\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  91  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.029386370489373804\n",
            "   Test F-score avg:  0.5026025659697465\n",
            "   Test F-score min:  0.4731421662655954\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  92  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028737068874761462\n",
            "   Test F-score avg:  0.44296584131304384\n",
            "   Test F-score min:  0.44296584131304384\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  93  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028668113891035317\n",
            "   Test F-score avg:  0.4911279819184952\n",
            "   Test F-score min:  0.44296584131304384\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  94  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0297725522890687\n",
            "   Test F-score avg:  0.49816367985594745\n",
            "   Test F-score min:  0.44296584131304384\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  95  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028501346660777926\n",
            "   Test F-score avg:  0.4931774240921377\n",
            "   Test F-score min:  0.44296584131304384\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  96  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0281101404922083\n",
            "   Test F-score avg:  0.4400376166962447\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  97  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028093986539170147\n",
            "   Test F-score avg:  0.4995335558558306\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  98  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028427336690947415\n",
            "   Test F-score avg:  0.507857600735821\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  99  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.030425113067030905\n",
            "   Test F-score avg:  0.4866712408700353\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  100  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028474713675677776\n",
            "   Test F-score avg:  0.5183105281024534\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  101  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028890476422384383\n",
            "   Test F-score avg:  0.49100739272291055\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  102  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02738196779973805\n",
            "   Test F-score avg:  0.4969583822218938\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  103  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028134243050590157\n",
            "   Test F-score avg:  0.5118093446049297\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  104  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.027690579229965807\n",
            "   Test F-score avg:  0.5076202025807622\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  105  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02753361735958606\n",
            "   Test F-score avg:  0.5119983913705702\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  106  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02761427001096308\n",
            "   Test F-score avg:  0.4881790098604892\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  107  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.026588945742696525\n",
            "   Test F-score avg:  0.49359310798357037\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  108  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.028068926418200135\n",
            "   Test F-score avg:  0.4785655974939946\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  109  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.026643617311492562\n",
            "   Test F-score avg:  0.5037012974502798\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  110  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.026548791211098433\n",
            "   Test F-score avg:  0.4647983644257545\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  111  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02681940540205687\n",
            "   Test F-score avg:  0.4823921010771389\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  112  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02670728275552392\n",
            "   Test F-score avg:  0.5047807565585136\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  113  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02562139336951077\n",
            "   Test F-score avg:  0.4935614308524464\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  114  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025306596164591612\n",
            "   Test F-score avg:  0.49347736057345815\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  115  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02555268134456128\n",
            "   Test F-score avg:  0.4699536018050832\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  116  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02518844292499125\n",
            "   Test F-score avg:  0.4597238509805942\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  117  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025944486632943152\n",
            "   Test F-score avg:  0.5182620466149235\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  118  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.027213783003389835\n",
            "   Test F-score avg:  0.49077719972850986\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  119  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025812326278537513\n",
            "   Test F-score avg:  0.47336451840222393\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  120  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025195069890469313\n",
            "   Test F-score avg:  0.4970026758190679\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  121  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025016454164870083\n",
            "   Test F-score avg:  0.48392265226826414\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  122  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02555242581292987\n",
            "   Test F-score avg:  0.4931058541835759\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  123  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02530431335326284\n",
            "   Test F-score avg:  0.5022309848456619\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  124  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02518003326840699\n",
            "   Test F-score avg:  0.49069609831631283\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  125  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024933631438761948\n",
            "   Test F-score avg:  0.47968553017504406\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  126  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02453368215356022\n",
            "   Test F-score avg:  0.4930884495585296\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  127  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024087006226181984\n",
            "   Test F-score avg:  0.4469740758379567\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  128  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024342783936299385\n",
            "   Test F-score avg:  0.49291741262466504\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  129  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024490570556372403\n",
            "   Test F-score avg:  0.49514299581097065\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  130  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023956740414723753\n",
            "   Test F-score avg:  0.4698480833736567\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  131  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024540622299537063\n",
            "   Test F-score avg:  0.48815740000691854\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  132  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025887322053313254\n",
            "   Test F-score avg:  0.47939829317991317\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  133  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025464140833355485\n",
            "   Test F-score avg:  0.4944230704234684\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  134  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025088097667321563\n",
            "   Test F-score avg:  0.49193711804833684\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  135  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02550112414173782\n",
            "   Test F-score avg:  0.4808903385898886\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  136  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025875578774139284\n",
            "   Test F-score avg:  0.47865450664421366\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  137  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.025570929353125393\n",
            "   Test F-score avg:  0.4695928972902088\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  138  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024571845028549432\n",
            "   Test F-score avg:  0.49082606364551423\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  139  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023909814865328373\n",
            "   Test F-score avg:  0.4731924477363811\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  140  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02379430897999555\n",
            "   Test F-score avg:  0.5044131028841832\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  141  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.026095064543187618\n",
            "   Test F-score avg:  0.487480632124084\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  142  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02368349318858236\n",
            "   Test F-score avg:  0.5021064794368961\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  143  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02333106161095202\n",
            "   Test F-score avg:  0.5127413894823338\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  144  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02318035049829632\n",
            "   Test F-score avg:  0.5081134416500739\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  145  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0228725953027606\n",
            "   Test F-score avg:  0.48040886605810007\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  146  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02241995434742421\n",
            "   Test F-score avg:  0.45091705747989064\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  147  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0223785167792812\n",
            "   Test F-score avg:  0.49150442517331205\n",
            "   Test F-score min:  0.4400376166962447\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  148  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.024178143846802412\n",
            "   Test F-score avg:  0.43152972296850417\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  149  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02467388166114688\n",
            "   Test F-score avg:  0.5045785852135002\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  150  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02352753027807921\n",
            "   Test F-score avg:  0.5084152333712092\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  151  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02241712047252804\n",
            "   Test F-score avg:  0.49165714126484994\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  152  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022342210356146096\n",
            "   Test F-score avg:  0.48292395558200063\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  153  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021907955571077765\n",
            "   Test F-score avg:  0.4831480057126291\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  154  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02174270318355411\n",
            "   Test F-score avg:  0.4547970602642346\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  155  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022770683700218796\n",
            "   Test F-score avg:  0.47671700194179156\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  156  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023548931907862426\n",
            "   Test F-score avg:  0.46008884768713376\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  157  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023823917983099817\n",
            "   Test F-score avg:  0.5069334548604577\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  158  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023923069844022392\n",
            "   Test F-score avg:  0.5051707496224103\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  159  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.023847094271332026\n",
            "   Test F-score avg:  0.4601945471286772\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  160  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02233598567545414\n",
            "   Test F-score avg:  0.45099558989625005\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  161  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022704598587006332\n",
            "   Test F-score avg:  0.4900366483185339\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  162  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02229999073315412\n",
            "   Test F-score avg:  0.4974674269469811\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  163  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02182263215072453\n",
            "   Test F-score avg:  0.4864007751798594\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  164  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021062803058885037\n",
            "   Test F-score avg:  0.48253454502391957\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  165  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021346429968252777\n",
            "   Test F-score avg:  0.48629883458163203\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  166  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021427043550647795\n",
            "   Test F-score avg:  0.4853596546387736\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  167  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02114298930391669\n",
            "   Test F-score avg:  0.4778590750602686\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  168  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020819202135317027\n",
            "   Test F-score avg:  0.45242184090309046\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  169  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022119815251789986\n",
            "   Test F-score avg:  0.46891537260903693\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  170  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02178474545944482\n",
            "   Test F-score avg:  0.4724500780932094\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  171  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02147333575412631\n",
            "   Test F-score avg:  0.4860742581251614\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  172  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021381147531792523\n",
            "   Test F-score avg:  0.5130685206449666\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  173  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02162082432769239\n",
            "   Test F-score avg:  0.486332468927388\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  174  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021260362537577748\n",
            "   Test F-score avg:  0.4794736599068883\n",
            "   Test F-score min:  0.43152972296850417\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  175  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020919963624328373\n",
            "   Test F-score avg:  0.4246922191255137\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  176  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02028793019708246\n",
            "   Test F-score avg:  0.4635092807169471\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  177  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02002669086214155\n",
            "   Test F-score avg:  0.46302690642104577\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  178  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020283527742139994\n",
            "   Test F-score avg:  0.4934580198196038\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  179  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020037154480814934\n",
            "   Test F-score avg:  0.4733022176821736\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  180  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020015238004270942\n",
            "   Test F-score avg:  0.44543348506186237\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  181  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.019969713338650764\n",
            "   Test F-score avg:  0.4742895386088727\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  182  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020241654361598194\n",
            "   Test F-score avg:  0.46341138089683065\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  183  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.01983742481097579\n",
            "   Test F-score avg:  0.48118788900729303\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  184  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.01966820766683668\n",
            "   Test F-score avg:  0.46070763457703895\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  185  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02037661967333406\n",
            "   Test F-score avg:  0.45424502959283675\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  186  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020977625716477634\n",
            "   Test F-score avg:  0.434800946870038\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  187  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022167412983253598\n",
            "   Test F-score avg:  0.4869287410145199\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  188  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021877790405414998\n",
            "   Test F-score avg:  0.492100963980943\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  189  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022434428753331303\n",
            "   Test F-score avg:  0.5034939020005261\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  190  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.020979259558953346\n",
            "   Test F-score avg:  0.507372471615821\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  191  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.0205532590392977\n",
            "   Test F-score avg:  0.46082245480816064\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  192  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.019964363786857574\n",
            "   Test F-score avg:  0.48492441557265914\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  193  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.01915385831380263\n",
            "   Test F-score avg:  0.4763410218006796\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  194  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.01946406523929909\n",
            "   Test F-score avg:  0.4763001030678061\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  195  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.021129861916415395\n",
            "   Test F-score avg:  0.44756001263738926\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  196  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.02110566459596157\n",
            "   Test F-score avg:  0.4693862414871611\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  197  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.022998087434098126\n",
            "   Test F-score avg:  0.4987527672704748\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  198  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.01993560343980789\n",
            "   Test F-score avg:  0.4711466216670813\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  199  out of total:  300\n",
            "----------------------------------------------------------------------\n",
            "   Train loss:  0.019925085850991307\n",
            "   Test F-score avg:  0.49780792398421425\n",
            "   Test F-score min:  0.4246922191255137\n",
            "   Test F-score max:  0.5840793909780575\n",
            "----------------------------------------------------------------------\n",
            "Epoch  200  out of total:  300\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 643, in <module>\n",
            "    train(params_obj)\n",
            "  File \"train.py\", line 575, in train\n",
            "  File \"train.py\", line 311, in train\n",
            "    y, _ = self.model([seq1,seq2,seq3, seq4],seq_len) # for four source of feature Xo, Xr, Xf\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/MSVA/src/msva_models.py\", line 79, in forward\n",
            "    y, att_weights = self.att1_3(x)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/drive/MyDrive/MSVA/src/msva_models.py\", line 35, in forward\n",
            "    Q = self.Q(x)  \n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\", line 94, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\", line 1753, in linear\n",
            "    return torch._C._nn.linear(input, weight, bias)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train.py -params parameters.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64WZiecIBcz9",
        "outputId": "164f2c42-b88c-4ec0-b5a9-ea9ae3419221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Argument are: ['inference.py', '-h5Dataset', 'SampleVideo.h5', '-video_name', 'JungleVideo', '-model_weight', 'model_weights/summe_random_non_overlap_0.5359.tar.pth']\n",
            "-model_weight:  model_weights/summe_random_non_overlap_0.5359.tar.pth\n",
            "-video_name:  JungleVideo\n",
            "-h5Dataset:  SampleVideo.h5\n",
            "video name found and corresponding index:  JungleVideo\n",
            "reshape prior: (1058, 2048)\n",
            "shape:  (2116, 1024)\n",
            "feature source 1 shape:  torch.Size([1, 2116, 1024])\n",
            "output yArray:  [0.1533187, 0.16194758, 0.17408434, 0.1746271, 0.18523388, 0.18848935, 0.17111611, 0.1524021, 0.16863762, 0.17604113, 0.16353074, 0.179725, 0.17347302, 0.15170436, 0.13815825, 0.14109433, 0.1164047, 0.121153176, 0.13111441, 0.12648325, 0.10195454, 0.100942574, 0.101195514, 0.12749544, 0.13816702, 0.14146492, 0.15226749, 0.15587264, 0.12091174, 0.14835978, 0.14930144, 0.15080829, 0.16132674, 0.17406191, 0.13000482, 0.13182041, 0.14251956, 0.118945315, 0.10588769, 0.10460448, 0.09862334, 0.07132334, 0.077815056, 0.10869694, 0.11520173, 0.113493904, 0.14089157, 0.14636552, 0.14023301, 0.1339958, 0.13979356, 0.112673536, 0.11466418, 0.09352295, 0.101903364, 0.10461036, 0.13182558, 0.113966964, 0.14267728, 0.15697512, 0.17228174, 0.16340393, 0.16667746, 0.16175187, 0.1468052, 0.14921302, 0.14014882, 0.14185026, 0.12624964, 0.11733945, 0.10115123, 0.10059563, 0.11718068, 0.13456652, 0.14544794, 0.13314319, 0.14011464, 0.12035887, 0.12432311, 0.1362563, 0.13742343, 0.14962521, 0.15121777, 0.14079997, 0.14198737, 0.14358012, 0.12103474, 0.13000071, 0.1372739, 0.11434756, 0.13251637, 0.14271933, 0.13262202, 0.10437037, 0.099470295, 0.08438991, 0.086548984, 0.08202696, 0.11301217, 0.115442015, 0.12286534, 0.10494034, 0.11687771, 0.10713979, 0.10880406, 0.106405854, 0.12342511, 0.12548102, 0.10330598, 0.09832664, 0.0974077, 0.10339804, 0.10732031, 0.12348759, 0.121758796, 0.121963106, 0.10780078, 0.10726714, 0.11787238, 0.11839477, 0.115633704, 0.119396545, 0.10566689, 0.09794493, 0.10101799, 0.09450223, 0.10343118, 0.10782601, 0.096561626, 0.1010648, 0.11342915, 0.094370544, 0.08458031, 0.075813584, 0.07927338, 0.10392964, 0.10049681, 0.1100091, 0.12921467, 0.14129639, 0.13296914, 0.14970315, 0.14953227, 0.15974787, 0.14059718, 0.13253132, 0.12854736, 0.13878569, 0.12387501, 0.13635409, 0.14908656, 0.14444587, 0.13091525, 0.13088016, 0.14365374, 0.12946412, 0.12795672, 0.15509453, 0.14785321, 0.12196989, 0.12406564, 0.15284738, 0.13866995, 0.14808948, 0.1565403, 0.15351668, 0.14062545, 0.12386425, 0.13604364, 0.1290249, 0.11957599, 0.10965868, 0.11144111, 0.09905948, 0.10112169, 0.10416722, 0.10621752, 0.10858486, 0.11106922, 0.114423536, 0.12897107, 0.122863725, 0.13030677, 0.12486316, 0.12354686, 0.116245806, 0.17097275, 0.1832334, 0.17685446, 0.19106308, 0.18216316, 0.13001512, 0.12238107, 0.123494685, 0.10886115, 0.1115221, 0.10819875, 0.11724796, 0.12718354, 0.1280546, 0.12153079, 0.14783853, 0.14778619, 0.13894226, 0.13386413, 0.1362842, 0.12588581, 0.10809231, 0.10758813, 0.116367504, 0.11187923, 0.11193228, 0.11789124, 0.11257015, 0.10777952, 0.12910955, 0.11929889, 0.12947682, 0.1537911, 0.16660461, 0.18305984, 0.17340171, 0.17341562, 0.14289658, 0.12511286, 0.09386458, 0.108781025, 0.08403658, 0.09495704, 0.10067712, 0.10286214, 0.09411007, 0.09723272, 0.107162856, 0.12085924, 0.12485939, 0.12276016, 0.13395634, 0.15240529, 0.1505184, 0.16682868, 0.16766492, 0.16574003, 0.15826541, 0.14550619, 0.12811844, 0.13052627, 0.1328463, 0.110482, 0.11093812, 0.118275955, 0.12528649, 0.13337854, 0.13596901, 0.1675916, 0.14480568, 0.1544601, 0.14079052, 0.16083303, 0.12984101, 0.13855396, 0.12070264, 0.1357176, 0.121769786, 0.1362941, 0.13713923, 0.13504753, 0.13703398, 0.14676654, 0.13304119, 0.1380519, 0.13760819, 0.11277906, 0.09387868, 0.10041833, 0.08769035, 0.09751181, 0.11763887, 0.13874416, 0.13515313, 0.16162628, 0.1634523, 0.15836081, 0.14560795, 0.14337179, 0.14429848, 0.14557287, 0.15029119, 0.14008772, 0.15203223, 0.12474535, 0.122118734, 0.13294098, 0.15011372, 0.14802685, 0.14973392, 0.1494163, 0.14932427, 0.14903976, 0.16443472, 0.17441168, 0.18379654, 0.19277012, 0.17108896, 0.1437142, 0.13504168, 0.13679853, 0.121763945, 0.14364693, 0.2006397, 0.21605735, 0.20100775, 0.20138416, 0.19231275, 0.14180031, 0.13211441, 0.13187118, 0.11593255, 0.113585934, 0.10196295, 0.09815119, 0.10955083, 0.10571687, 0.09942283, 0.101284444, 0.1078591, 0.08633022, 0.1014532, 0.120085225, 0.12248441, 0.12502371, 0.13962872, 0.11687897, 0.10327242, 0.10313859, 0.09720412, 0.10725472, 0.13696301, 0.13591461, 0.1325889, 0.12137065, 0.09661601, 0.085834816, 0.08849301, 0.10678973, 0.103679396, 0.11735846, 0.110484995, 0.102149166, 0.08699659, 0.105516456, 0.098860934, 0.096457034, 0.108413145, 0.11261833, 0.14333935, 0.15199327, 0.1491591, 0.14336282, 0.13263738, 0.112496994, 0.09365271, 0.09643783, 0.09722695, 0.12916538, 0.116633035, 0.1514952, 0.15768379, 0.16010073, 0.13627435, 0.1322577, 0.11758196, 0.10196205, 0.09510451, 0.09968066, 0.11552272, 0.10042276, 0.121556856, 0.13981101, 0.13238235, 0.11070724, 0.13055305, 0.13005054, 0.1115336, 0.10474686, 0.12971255, 0.13186416, 0.12369511, 0.14565054, 0.14728597, 0.12989716, 0.11864214, 0.115577064, 0.10525987, 0.13437863, 0.1507299, 0.16846955, 0.17499934, 0.16332245, 0.14618869, 0.12839854, 0.104874745, 0.09395446, 0.1048921, 0.12898058, 0.13301662, 0.15093972, 0.1920816, 0.19771056, 0.15727833, 0.16322567, 0.13339952, 0.09439867, 0.0817063, 0.09772068, 0.09703166, 0.106960475, 0.1219828, 0.14137663, 0.14576484, 0.1298761, 0.1348336, 0.12625133, 0.1176883, 0.12083878, 0.120805085, 0.12865576, 0.12370131, 0.121939324, 0.117258646, 0.12827003, 0.117806956, 0.12441905, 0.11490341, 0.11146222, 0.10912488, 0.120907605, 0.12413027, 0.1271816, 0.12912488, 0.12434795, 0.10975244, 0.11937003, 0.10761019, 0.093342856, 0.099781714, 0.10116999, 0.08695053, 0.09778537, 0.111963615, 0.14250408, 0.14891748, 0.15272602, 0.17441668, 0.17818448, 0.15892676, 0.14544691, 0.13824506, 0.13749692, 0.12965748, 0.106745146, 0.117375985, 0.11511277, 0.117802344, 0.117411956, 0.11920227, 0.12304499, 0.13546813, 0.116544984, 0.10125675, 0.11369723, 0.113700375, 0.10884722, 0.10921101, 0.13157904, 0.1301972, 0.12344436, 0.123105824, 0.11961552, 0.12063823, 0.11566323, 0.11386204, 0.11335249, 0.13080546, 0.13746743, 0.13864246, 0.14417076, 0.17459872, 0.1654729, 0.14797746, 0.16981068, 0.18185394, 0.15238018, 0.14304449, 0.13938823, 0.1145853, 0.099080905, 0.106089555, 0.10525173, 0.11169578, 0.10474119, 0.10294565, 0.1009773, 0.10231207, 0.10292526, 0.12756167, 0.13018309, 0.11336704, 0.122938894, 0.117661856, 0.13134435, 0.14153084, 0.14380492, 0.1608828, 0.16151099, 0.14052206, 0.12777287, 0.1360689, 0.1093568, 0.11168425, 0.09432837, 0.08414058, 0.08749388, 0.09235664, 0.11132713, 0.14291097, 0.1695012, 0.16187836, 0.15575881, 0.15500788, 0.1408095, 0.12982759, 0.12250589, 0.14451599, 0.13104536, 0.11760087, 0.12205043, 0.13426761, 0.119584285, 0.12766556, 0.14226754, 0.15515926, 0.15042254, 0.14433786, 0.1533424, 0.16327314, 0.16323848, 0.16297731, 0.16708389, 0.15013249, 0.12946633, 0.13415486, 0.1267421, 0.12710701, 0.15377028, 0.16555016, 0.13231853, 0.14326687, 0.15385835, 0.12316692, 0.12113391, 0.15046616, 0.15055358, 0.14114009, 0.14792335, 0.13431954, 0.12945673, 0.12640779, 0.12555617, 0.12951347, 0.13296834, 0.117673315, 0.12431083, 0.12660131, 0.14424531, 0.14950451, 0.1444852, 0.14259027, 0.13342103, 0.11085949, 0.09897903, 0.099155985, 0.09424933, 0.094402626, 0.09178107, 0.09516938, 0.10264866, 0.110042766, 0.1200008, 0.10910485, 0.10718801, 0.10665433, 0.11016498, 0.089671455, 0.10255985, 0.11120297, 0.10309563, 0.0895978, 0.10931869, 0.120562516, 0.13650183, 0.15743625, 0.16819362, 0.14748393, 0.16214988, 0.1360322, 0.13486676, 0.13087712, 0.15958668, 0.12584445, 0.13792607, 0.12084575, 0.12144617, 0.11147213, 0.120965675, 0.1151332, 0.14255626, 0.14361826, 0.14590469, 0.13517924, 0.1330637, 0.101157226, 0.11298259, 0.1018378, 0.1092843, 0.11118863, 0.11837779, 0.09473565, 0.09591547, 0.08605874, 0.07813375, 0.09413336, 0.09372061, 0.119598076, 0.12839293, 0.14356911, 0.12654355, 0.1406722, 0.15899217, 0.15283103, 0.16849867, 0.16314468, 0.16284423, 0.117905214, 0.12207067, 0.11366248, 0.13613494, 0.12656671, 0.12914588, 0.13108063, 0.11902489, 0.11712091, 0.115144156, 0.115640715, 0.1380785, 0.1298228, 0.11768763, 0.13836102, 0.14554647, 0.12509231, 0.13910203, 0.13962656, 0.11626985, 0.10000501, 0.091185056, 0.087218866, 0.07852796, 0.08779411, 0.105244376, 0.13039935, 0.12007344, 0.12804535, 0.122176275, 0.12091398, 0.11016743, 0.11663853, 0.11653038, 0.11450615, 0.11896358, 0.13778535, 0.16102739, 0.17726585, 0.18616043, 0.17446904, 0.13677678, 0.10833303, 0.111609876, 0.11798094, 0.117159165, 0.147257, 0.14650314, 0.14776938, 0.13578992, 0.13797458, 0.123151444, 0.1262236, 0.10641436, 0.11525963, 0.13364002, 0.14339173, 0.1529443, 0.1433573, 0.18743056, 0.18084317, 0.18432918, 0.18077198, 0.20716326, 0.14944373, 0.14328837, 0.15271297, 0.15894814, 0.15061535, 0.14829417, 0.13209966, 0.113396205, 0.096829325, 0.112230085, 0.12106609, 0.12358662, 0.11067096, 0.122114934, 0.11233938, 0.115746714, 0.12604001, 0.14571479, 0.13521878, 0.14622462, 0.1472818, 0.16437408, 0.13858725, 0.14955212, 0.1163408, 0.11218395, 0.089608684, 0.10141301, 0.11098715, 0.1428279, 0.14650477, 0.14547347, 0.13874929, 0.12529208, 0.11104324, 0.112100735, 0.124118075, 0.11955575, 0.10929439, 0.10890965, 0.105979584, 0.09744497, 0.1007065, 0.11496995, 0.09469275, 0.094916895, 0.10052188, 0.12597999, 0.12096319, 0.14231253, 0.15534237, 0.16033527, 0.1603264, 0.18200603, 0.17151873, 0.14990973, 0.13594222, 0.14180762, 0.13839526, 0.14206792, 0.14283949, 0.15840968, 0.15818197, 0.14373602, 0.13417405, 0.13476565, 0.111632094, 0.10586858, 0.10038797, 0.120422825, 0.13060565, 0.1542572, 0.14129159, 0.14643402, 0.12735704, 0.13952537, 0.13672122, 0.14007945, 0.12702598, 0.13858071, 0.11058626, 0.109017, 0.103959225, 0.12550607, 0.1182216, 0.13270938, 0.1272223, 0.13111492, 0.121633366, 0.14709742, 0.13393843, 0.13802955, 0.14051089, 0.13723321, 0.10868939, 0.111101724, 0.11010323, 0.10285095, 0.09611587, 0.096173085, 0.109501794, 0.1009778, 0.089178346, 0.10713837, 0.1179687, 0.11216364, 0.12074033, 0.13038787, 0.115331724, 0.1141263, 0.11539732, 0.12432288, 0.1102461, 0.10350405, 0.09612794, 0.09720261, 0.07756128, 0.08325017, 0.12184346, 0.111494064, 0.12213681, 0.12947336, 0.14188758, 0.11796, 0.13859929, 0.13829187, 0.14604652, 0.16022852, 0.17087063, 0.16478525, 0.16383293, 0.15567335, 0.16469696, 0.15268412, 0.1433891, 0.14842966, 0.14075379, 0.11280121, 0.104288876, 0.12108376, 0.12190008, 0.13477746, 0.1457757, 0.1546363, 0.14267625, 0.13193047, 0.12305967, 0.11151731, 0.10189595, 0.09769444, 0.089476235, 0.092451245, 0.103950396, 0.11073687, 0.123185374, 0.12034454, 0.111898884, 0.11639343, 0.12072734, 0.10975592, 0.13003519, 0.14387485, 0.12244681, 0.11153926, 0.11639664, 0.105633356, 0.11198608, 0.15443423, 0.16204514, 0.15944971, 0.15983786, 0.14214739, 0.10568313, 0.10658793, 0.11914339, 0.120983526, 0.1235532, 0.13530645, 0.1282652, 0.13140942, 0.11718049, 0.1346015, 0.12421669, 0.13714619, 0.11592926, 0.119579986, 0.0953753, 0.0983027, 0.07964365, 0.08865113, 0.08030077, 0.10884893, 0.1075215, 0.13248369, 0.11941582, 0.1460329, 0.1290555, 0.12518422, 0.12002019, 0.13041861, 0.113484904, 0.10879298, 0.10420165, 0.0922616, 0.08622989, 0.08520299, 0.07709218, 0.102968276, 0.10784565, 0.13443835, 0.14020506, 0.14506856, 0.13571337, 0.16264936, 0.13965784, 0.168466, 0.1669198, 0.16281712, 0.13326553, 0.13575165, 0.118850015, 0.11317478, 0.106659375, 0.11812117, 0.11381592, 0.08630952, 0.11044128, 0.11336811, 0.12791683, 0.15497562, 0.16311215, 0.14928901, 0.14127418, 0.1208205, 0.09950111, 0.15861967, 0.15960659, 0.17430303, 0.17629167, 0.17934597, 0.11899992, 0.13396752, 0.114953175, 0.10454565, 0.12243287, 0.13361666, 0.1146341, 0.13207182, 0.1457741, 0.12900802, 0.121536955, 0.12673542, 0.13401382, 0.12529187, 0.124210395, 0.13838251, 0.13004084, 0.115462944, 0.11722722, 0.13745333, 0.12748101, 0.15117966, 0.15568635, 0.15726472, 0.15725866, 0.16645661, 0.14284344, 0.13282259, 0.125323, 0.098563075, 0.08884727, 0.10067745, 0.12919505, 0.14576718, 0.14481764, 0.14329334, 0.13132964, 0.110849604, 0.10073988, 0.1068459, 0.108303785, 0.13402522, 0.12921089, 0.13790318, 0.12755008, 0.13590603, 0.12537912, 0.13393459, 0.12709898, 0.12842032, 0.12452668, 0.10508279, 0.08798632, 0.08972833, 0.09001007, 0.093162976, 0.09104208, 0.10995261, 0.11009747, 0.13872696, 0.14205828, 0.16326794, 0.1510543, 0.14986853, 0.13148089, 0.1268951, 0.10495281, 0.106220305, 0.09465392, 0.07655303, 0.06145966, 0.07214793, 0.075761065, 0.08461291, 0.099975646, 0.10971607, 0.099860534, 0.09824868, 0.09432458, 0.104838274, 0.09622509, 0.11449029, 0.115673855, 0.11888347, 0.120334074, 0.13300952, 0.12189156, 0.12393482, 0.120288394, 0.12460448, 0.11282519, 0.113133155, 0.12782666, 0.1250523, 0.10113078, 0.10743538, 0.10799326, 0.08554274, 0.08627895, 0.11011913, 0.10947041, 0.121976495, 0.11655919, 0.12882978, 0.12199297, 0.13085838, 0.120990254, 0.12848845, 0.11134229, 0.101545595, 0.09036343, 0.09127818, 0.100517035, 0.11360309, 0.09999807, 0.118181385, 0.12786755, 0.15316255, 0.13723044, 0.1385292, 0.1412613, 0.14857438, 0.11299224, 0.1391391, 0.16514893, 0.15756604, 0.13154808, 0.13531308, 0.1185051, 0.11990698, 0.09859241, 0.1187291, 0.116923586, 0.11848035, 0.1056577, 0.11772027, 0.11385459, 0.12293591, 0.1478645, 0.1471159, 0.14073497, 0.14718354, 0.16887662, 0.16531149, 0.16571149, 0.17190447, 0.146332, 0.1497992, 0.11791736, 0.13897622, 0.16920629, 0.17613062, 0.1466217, 0.15542433, 0.13213614, 0.108353496, 0.12688781, 0.13390882, 0.1492073, 0.16998145, 0.17221515, 0.15404634, 0.14472517, 0.15177816, 0.13007835, 0.113726415, 0.1293715, 0.14390284, 0.14389476, 0.13811612, 0.14646588, 0.14177807, 0.14394008, 0.11797655, 0.11169356, 0.1037434, 0.08936797, 0.075219505, 0.074582815, 0.09589469, 0.11804086, 0.14673305, 0.13814959, 0.14213297, 0.12610225, 0.10624646, 0.09067171, 0.10908071, 0.11399702, 0.14911008, 0.14611712, 0.16887066, 0.16889688, 0.16862474, 0.15158783, 0.16170678, 0.12832852, 0.10978036, 0.113323726, 0.09876751, 0.08921982, 0.13911484, 0.14145228, 0.15302345, 0.14367062, 0.14241214, 0.112178765, 0.11736016, 0.12174787, 0.13516517, 0.13848351, 0.12397631, 0.13841107, 0.13454305, 0.13542654, 0.14248535, 0.14589879, 0.16274376, 0.13671264, 0.1309261, 0.13874781, 0.1395268, 0.093350045, 0.10350321, 0.10163566, 0.10442741, 0.105732635, 0.15027484, 0.16770625, 0.17182012, 0.1634115, 0.1588255, 0.13632503, 0.11489707, 0.11786357, 0.12380125, 0.12797871, 0.12283365, 0.13315186, 0.1290448, 0.11504402, 0.10842178, 0.117387556, 0.115320444, 0.13477442, 0.13108583, 0.12684867, 0.14113458, 0.14309657, 0.1419107, 0.13559334, 0.15300226, 0.12346093, 0.107734464, 0.09213325, 0.11052562, 0.09658657, 0.10505805, 0.110575244, 0.121924, 0.10520846, 0.10998269, 0.118078604, 0.12781365, 0.10685458, 0.13817695, 0.13513431, 0.1462867, 0.12754762, 0.15560755, 0.13417736, 0.14300232, 0.123838976, 0.1364173, 0.10962276, 0.10962726, 0.10976221, 0.09823423, 0.09788219, 0.10584996, 0.107149825, 0.13715038, 0.1394716, 0.14942175, 0.1397408, 0.13701534, 0.10902224, 0.115815245, 0.11708087, 0.13692918, 0.14357993, 0.15140574, 0.15749097, 0.14485624, 0.13461027, 0.14354832, 0.14435017, 0.13701747, 0.12997863, 0.12191077, 0.094689265, 0.1072742, 0.09936407, 0.107928835, 0.103686236, 0.11732986, 0.09224667, 0.11021783, 0.12283585, 0.14802063, 0.14463046, 0.14013432, 0.117077515, 0.10688982, 0.097287275, 0.1145761, 0.10200982, 0.10208049, 0.10301781, 0.12147169, 0.10770001, 0.112859175, 0.12246375, 0.10931493, 0.08027801, 0.102730416, 0.12433831, 0.13622694, 0.16112329, 0.16551527, 0.1442562, 0.13377212, 0.12818673, 0.12216123, 0.11729336, 0.13029413, 0.11581707, 0.121268965, 0.11174512, 0.15586053, 0.14115426, 0.15685895, 0.15652266, 0.15780075, 0.11856177, 0.12683846, 0.11793846, 0.10761418, 0.10362492, 0.103083946, 0.10815672, 0.107658945, 0.106474325, 0.12035642, 0.1354943, 0.11492081, 0.13933425, 0.15103994, 0.1365032, 0.13919489, 0.18041357, 0.15285158, 0.1405972, 0.15642786, 0.14734982, 0.099622115, 0.1359615, 0.15852387, 0.14583763, 0.13775663, 0.15447405, 0.13361916, 0.12766321, 0.124412574, 0.13771467, 0.1232063, 0.11758085, 0.09404531, 0.10587342, 0.15727201, 0.17400244, 0.18507434, 0.18866989, 0.19808285, 0.13634118, 0.13390808, 0.12033455, 0.13195562, 0.10770432, 0.1095343, 0.10708795, 0.113769785, 0.11623579, 0.124935925, 0.12572293, 0.14648631, 0.1439933, 0.15431714, 0.15010273, 0.1579257, 0.13732459, 0.13522865, 0.11353288, 0.110147335, 0.10058836, 0.09961981, 0.09296938, 0.102851406, 0.11305668, 0.11445911, 0.12543488, 0.12847337, 0.122370385, 0.1391965, 0.14821751, 0.13376501, 0.13648078, 0.13972339, 0.1050513, 0.101060495, 0.11299869, 0.11670335, 0.111902, 0.12197205, 0.119188525, 0.11703006, 0.12665018, 0.11849205, 0.12932184, 0.13766417, 0.13074118, 0.11895116, 0.13037995, 0.12490438, 0.13788788, 0.15564546, 0.15920374, 0.14696708, 0.13615438, 0.11207441, 0.09216479, 0.088850975, 0.08266826, 0.09543477, 0.088627554, 0.109510675, 0.12474573, 0.14330783, 0.13328621, 0.14301445, 0.12490277, 0.12841578, 0.13145065, 0.13289674, 0.12381408, 0.13028184, 0.115368865, 0.10887556, 0.109876774, 0.12316521, 0.11058476, 0.099435784, 0.10184517, 0.10289453, 0.10020391, 0.1004737, 0.10694946, 0.1437954, 0.16762507, 0.1601737, 0.198492, 0.19241832, 0.15546362, 0.16612627, 0.15972939, 0.14594933, 0.14072207, 0.13849306, 0.10367087, 0.118921325, 0.097707614, 0.10484395, 0.11065445, 0.11830642, 0.115471505, 0.11430918, 0.12719242, 0.11934267, 0.11293206, 0.114380695, 0.1301016, 0.11591524, 0.117353246, 0.1618075, 0.14810777, 0.14219013, 0.15062594, 0.15340328, 0.11505165, 0.11920722, 0.11865568, 0.16385688, 0.15562078, 0.15116265, 0.17182009, 0.1838155, 0.18368313, 0.18171522, 0.17370224, 0.15509775, 0.16661344, 0.1341308, 0.15056214, 0.18119648, 0.17095575, 0.14396764, 0.121582404, 0.12572068, 0.09121989, 0.11084151, 0.11918416, 0.12863624, 0.14456013, 0.15265104, 0.15014963, 0.16946109, 0.17180246, 0.16241924, 0.1699701, 0.17141077, 0.13762295, 0.13662894, 0.12125349, 0.12568837, 0.13400093, 0.13618213, 0.14254312, 0.13027674, 0.11815975, 0.093471214, 0.08980525, 0.0873325, 0.08923535, 0.10562603, 0.12271438, 0.12136303, 0.10435295, 0.12570447, 0.10298626, 0.08790997, 0.09220853, 0.11780403, 0.0941628, 0.10807159, 0.1516217, 0.15131487, 0.13690342, 0.15305126, 0.14962639, 0.12677664, 0.13123766, 0.14121065, 0.12820484, 0.13285309, 0.13141629, 0.12376443, 0.11394687, 0.11937964, 0.11579436, 0.11968194, 0.12122836, 0.12822899, 0.13884845, 0.14939639, 0.12649417, 0.13358048, 0.13045797, 0.119798616, 0.10514519, 0.13077043, 0.12984055, 0.13039297, 0.13092497, 0.15143025, 0.12570012, 0.13553411, 0.13912272, 0.14089209, 0.13135931, 0.13896067, 0.12679242, 0.11382399, 0.11863609, 0.10206188, 0.10394076, 0.094950005, 0.11001768, 0.10928483, 0.12940255, 0.13293898, 0.13933514, 0.12855075, 0.119850695, 0.113804445, 0.11187063, 0.13530953, 0.14869554, 0.18099359, 0.18371846, 0.18286212, 0.16521487, 0.16895193, 0.15282069, 0.15418027, 0.15902166, 0.16081175, 0.13273138, 0.12063815, 0.10825859, 0.09453849, 0.089681305, 0.0889219, 0.0779062, 0.07793085, 0.078628734, 0.09790395, 0.09586586, 0.109100655, 0.10329366, 0.10512688, 0.08239541, 0.10234352, 0.08423264, 0.09735427, 0.088709965, 0.11423497, 0.109460354, 0.11568655, 0.11329697, 0.14079478, 0.1388094, 0.12759267, 0.15038022, 0.16087607, 0.14203015, 0.12991467, 0.15096588, 0.12638624, 0.11345087, 0.14158042, 0.1274294, 0.14059374, 0.14554358, 0.1574303, 0.124365345, 0.16228515, 0.12687787, 0.14406121, 0.16732016, 0.18139568, 0.15560302, 0.18208829, 0.18983153, 0.15845516, 0.14514753, 0.13795844, 0.14904574, 0.12732264, 0.14259341, 0.15728001, 0.15550946, 0.11613128, 0.116906226, 0.09152138, 0.069964565, 0.08571589, 0.11797185, 0.11299529, 0.16204265, 0.18912552, 0.20058572, 0.18911552, 0.21281338, 0.17603981, 0.2036551, 0.19830127, 0.18051589, 0.15125874, 0.14471331, 0.107354864, 0.1002176, 0.107883014, 0.12879795, 0.13577497, 0.13995102, 0.18129864, 0.20823905, 0.20209472, 0.19998685, 0.19912753, 0.15601672, 0.1409933, 0.1482847, 0.15279213, 0.14822707, 0.14123192, 0.12525037, 0.10498649, 0.08783755, 0.10056035, 0.09665801, 0.12583265, 0.1568858, 0.16127852, 0.1406784, 0.14963432, 0.14101231, 0.10778044, 0.121667385, 0.16901974, 0.16864678, 0.14332965, 0.16062367, 0.15634504, 0.10526508, 0.117696986, 0.14556912, 0.13789408, 0.13463202, 0.13573804, 0.13151161, 0.15025666, 0.14900707, 0.14769682, 0.16006485, 0.161818, 0.14036497, 0.14794026, 0.13775441, 0.13678327, 0.118798114, 0.10542307, 0.10909362, 0.12264869, 0.12954286, 0.14364275, 0.1550838, 0.14205882, 0.12807137, 0.13877264, 0.14035825, 0.13040273, 0.1543453, 0.17870152, 0.150853, 0.15151484, 0.13998355, 0.115233466, 0.09104615, 0.09613391, 0.07081338, 0.07964966, 0.07092567, 0.0714419, 0.06222573, 0.06025686, 0.04921794, 0.05352061, 0.07436762, 0.079517856, 0.106248476, 0.13240273, 0.13864556, 0.11857946, 0.13352574, 0.15286043, 0.1256711, 0.16266519, 0.17855838, 0.18464872, 0.15674911, 0.20130005, 0.19811451, 0.22600189, 0.21623385, 0.22379422, 0.20740345, 0.17641878, 0.16384742, 0.15415308, 0.14502963, 0.12281746, 0.1486578, 0.13693431, 0.14709088, 0.14782324, 0.16847691, 0.15993297, 0.15005814, 0.14252107, 0.1619136, 0.1359218, 0.11661255, 0.11198942, 0.12902828, 0.121766604, 0.12567423, 0.1207808, 0.11828218, 0.1011395, 0.07649503, 0.079388134, 0.090213925, 0.09291528, 0.09468773, 0.11140468, 0.10782931, 0.09675024, 0.116573215, 0.12588562, 0.10769615, 0.1278741, 0.13268307, 0.114441514, 0.10755243, 0.13021323, 0.110945165, 0.10938213, 0.103970505, 0.096413545, 0.09569579, 0.11524773, 0.10987075, 0.10999791, 0.123278834, 0.11490625, 0.09167682, 0.108353004, 0.11393823, 0.11991918, 0.12458734, 0.13541426, 0.14286101, 0.14279887, 0.12150852, 0.11142771, 0.107518114, 0.09315814, 0.11683885, 0.13961935, 0.13294868, 0.13926151, 0.15203986, 0.13026127, 0.107141234, 0.11556008, 0.1247071, 0.10669222, 0.0929618, 0.11767058, 0.11580936, 0.095248505, 0.11203188, 0.15376815, 0.12464652, 0.15054035, 0.14276513, 0.1746352, 0.14425342, 0.16534828, 0.13539356, 0.15454674, 0.11155635, 0.11829605, 0.10921153, 0.10672877, 0.123759225, 0.13728264, 0.12805036, 0.121208265, 0.15928134, 0.15783402, 0.15327474, 0.16272323, 0.17012176, 0.1552116, 0.13466392, 0.12172332, 0.11643802, 0.11319566, 0.102448836, 0.10447937, 0.12871964, 0.12416743, 0.13217063, 0.12416385, 0.12152301, 0.0957821, 0.11185537, 0.09757193, 0.09686522, 0.1110566, 0.1021687, 0.09146533, 0.09376998, 0.099569894, 0.0937739, 0.10399847, 0.10024633, 0.10480074, 0.11762009, 0.104697324, 0.11376993, 0.12391852, 0.12379731, 0.13891263, 0.15637943, 0.16178453, 0.15200865, 0.18667445, 0.16351299, 0.14195555, 0.15280242, 0.16029665, 0.12132422, 0.11271268, 0.15236315, 0.13141958, 0.13336179, 0.14126457, 0.16137803, 0.12039648, 0.13884695, 0.13384277, 0.1253974, 0.109749794, 0.12834685, 0.12262998, 0.13711694, 0.13566682, 0.14510997, 0.14166465, 0.16368894, 0.14322008, 0.14163001, 0.12826559, 0.13977301, 0.0928287, 0.09642856, 0.10218998, 0.1049029, 0.082600035, 0.09599302, 0.12008245, 0.13676548, 0.1468155, 0.1755249, 0.18813127, 0.16020826, 0.14354452, 0.12089841, 0.09483808, 0.07021026, 0.078478396, 0.0887094, 0.09097132, 0.10372038, 0.11991344, 0.12129466, 0.13303223, 0.14178953, 0.14167036, 0.13005015, 0.11836892, 0.11586161, 0.11925477, 0.10419606, 0.13829412, 0.15591171, 0.13872793, 0.13602099, 0.131406, 0.09867512, 0.09224554, 0.12467103, 0.14036927, 0.15645446, 0.17372073, 0.18758178, 0.16763152, 0.15375927, 0.17325339, 0.16104563, 0.16069083, 0.14689207, 0.14657801, 0.11827256, 0.10915518, 0.08345671, 0.090769276, 0.12257071, 0.13553213, 0.14686246, 0.14526722, 0.1459287, 0.10910405, 0.12364379, 0.12451927, 0.12846684, 0.121990666, 0.122902155, 0.10759716, 0.11359773, 0.12875988, 0.12220873, 0.11621954, 0.10602991, 0.11927996, 0.10212727, 0.11392583, 0.1277236, 0.15203449, 0.15188284, 0.15802181, 0.1500996, 0.14992532, 0.12593544, 0.10939477, 0.11613891, 0.114934966, 0.09806068, 0.10125981, 0.07940538, 0.10404532, 0.10915849, 0.15219383, 0.1436092, 0.1549876, 0.12996273, 0.13525048, 0.102057025, 0.113290645, 0.12529561, 0.12432647, 0.12564154, 0.13913871, 0.13561283, 0.15283492, 0.14466313, 0.14732198, 0.13439703, 0.13524191, 0.10525606, 0.13370796, 0.12470929, 0.12558925, 0.12627043, 0.13315108, 0.10622622, 0.11874466, 0.117828906, 0.10914464, 0.104675256, 0.11132177, 0.09782169, 0.10312335, 0.108731285, 0.115272775, 0.10514152, 0.124950126, 0.112224415, 0.13350928, 0.13707002, 0.14905393, 0.11701711, 0.14549027, 0.11339329, 0.11264567, 0.11857855, 0.14266379, 0.12526159, 0.12927823, 0.13964124, 0.1524669, 0.15801413, 0.16035253, 0.18385682, 0.19270208, 0.1609226, 0.18828678, 0.1997877, 0.19329868, 0.16025737, 0.16349381, 0.110210896, 0.09698856, 0.08424879, 0.11394061, 0.11175458, 0.12871125, 0.15670696, 0.18051276, 0.15590279, 0.18214568, 0.16635576, 0.15684178, 0.12916091, 0.14440209, 0.121493496, 0.13864127, 0.11749607, 0.13888851, 0.1243998, 0.13604137, 0.13371217, 0.15280448, 0.13789833, 0.15597367, 0.16670713, 0.19464989, 0.17881253, 0.17494968, 0.16152799, 0.14529888, 0.117273435, 0.12672088, 0.1483089, 0.14500096, 0.14833903, 0.15736492, 0.14478226, 0.1362609, 0.1263241, 0.11561634, 0.09866738, 0.0969092, 0.104692675, 0.12495092, 0.12371185, 0.15682779, 0.14967886, 0.12550575, 0.13749732, 0.12946, 0.1319563, 0.13041921, 0.1263136, 0.13336274, 0.13780157, 0.10516937, 0.1276398, 0.14492741, 0.1263798, 0.14979553, 0.16063637, 0.15676133, 0.1467173, 0.1343902, 0.12990023, 0.10564609, 0.09796242, 0.11341338, 0.11141901, 0.10268009]\n",
            "output yArray Shape:  2111\n",
            "0.1533187\n",
            "output yArray:  [0.15763314068317413, 0.17435571551322937, 0.18686160445213318, 0.16175910830497742, 0.17233937978744507, 0.17162787914276123, 0.1625886857509613, 0.13962629437446594, 0.11877893656492233, 0.12879882752895355, 0.10144855827093124, 0.1143454760313034, 0.13981597125530243, 0.15407006442546844, 0.13463576138019562, 0.15005487203598022, 0.1676943302154541, 0.13091261684894562, 0.1307324469089508, 0.10524608194828033, 0.08497334271669388, 0.09325599670410156, 0.11434781551361084, 0.1436285376548767, 0.13711440563201904, 0.1262335479259491, 0.10409356653690338, 0.10325686633586884, 0.12289626896381378, 0.14982619881629944, 0.167842835187912, 0.16421467065811157, 0.14800910651683807, 0.14099954068660736, 0.12179454416036606, 0.1008734256029129, 0.1258735954761505, 0.1392955631017685, 0.13023674488067627, 0.1302897036075592, 0.14352431893348694, 0.14600887894630432, 0.1427837461233139, 0.12551772594451904, 0.12581072747707367, 0.13761785626411438, 0.11849619448184967, 0.09193010628223419, 0.08428797125816345, 0.11422709375619888, 0.11390283703804016, 0.11200875043869019, 0.10760495811700821, 0.12445306777954102, 0.10081630945205688, 0.10040286928415298, 0.11540395021438599, 0.12186095118522644, 0.10753396153450012, 0.11813357472419739, 0.11751512438058853, 0.1018059104681015, 0.09776011109352112, 0.1056285947561264, 0.09881321340799332, 0.10389985144138336, 0.08019694685935974, 0.09160150587558746, 0.1052529513835907, 0.13525553047657013, 0.14133614301681519, 0.1546400785446167, 0.1365642547607422, 0.1336665153503418, 0.13011455535888672, 0.14676621556282043, 0.13089770078659058, 0.1365589201450348, 0.14152562618255615, 0.13491155207157135, 0.13845650851726532, 0.14337971806526184, 0.15502849221229553, 0.13224485516548157, 0.13253426551818848, 0.11461733281612396, 0.1052502989768982, 0.10264445841312408, 0.10740119218826294, 0.11274637281894684, 0.12591740489006042, 0.12758496403694153, 0.11989633738994598, 0.17710307240486145, 0.1839587688446045, 0.15608914196491241, 0.122937873005867, 0.11019162833690643, 0.11272335052490234, 0.12761907279491425, 0.13468466699123383, 0.14336422085762024, 0.1350741684436798, 0.11698906123638153, 0.11197781562805176, 0.11190575361251831, 0.11523069441318512, 0.1184445321559906, 0.12438785284757614, 0.16019785404205322, 0.17823077738285065, 0.15815609693527222, 0.10948872566223145, 0.09640879929065704, 0.0978170782327652, 0.09848611056804657, 0.1021977886557579, 0.12285931408405304, 0.12835824489593506, 0.15146183967590332, 0.16724678874015808, 0.16200271248817444, 0.13681231439113617, 0.1316862851381302, 0.11071005463600159, 0.12178122252225876, 0.13467377424240112, 0.156198650598526, 0.14762531220912933, 0.14533701539039612, 0.12962830066680908, 0.12874369323253632, 0.13671666383743286, 0.13604074716567993, 0.13990387320518494, 0.13783004879951477, 0.10332886874675751, 0.09405434131622314, 0.10757534205913544, 0.13694864511489868, 0.1625392884016037, 0.15198437869548798, 0.1438351273536682, 0.1479320228099823, 0.14605997502803802, 0.12343204021453857, 0.14152735471725464, 0.14888039231300354, 0.14937028288841248, 0.15673723816871643, 0.17910411953926086, 0.1819295436143875, 0.13937795162200928, 0.12928123772144318, 0.17214331030845642, 0.20853254199028015, 0.19684845209121704, 0.13695736229419708, 0.1239018589258194, 0.10777443647384644, 0.10385101288557053, 0.10256984829902649, 0.10457177460193634, 0.09389171004295349, 0.12128481268882751, 0.13232621550559998, 0.1100756973028183, 0.10017135739326477, 0.12210886180400848, 0.13425174355506897, 0.10899332910776138, 0.08716391026973724, 0.10523456335067749, 0.11392173171043396, 0.09457287937402725, 0.10218869149684906, 0.10243508964776993, 0.1279788315296173, 0.15057618916034698, 0.13800010085105896, 0.10307484865188599, 0.09683238714933395, 0.12289920449256897, 0.15458950400352478, 0.1481875479221344, 0.1249198317527771, 0.0985332801938057, 0.10760168731212616, 0.11098980903625488, 0.13609668612480164, 0.12063014507293701, 0.12079206854104996, 0.11722970753908157, 0.12777963280677795, 0.14646825194358826, 0.12426964938640594, 0.11041846871376038, 0.14255425333976746, 0.17173445224761963, 0.15475556254386902, 0.11663664132356644, 0.09942327439785004, 0.13099859654903412, 0.17151066660881042, 0.17749443650245667, 0.14831259846687317, 0.08805248141288757, 0.09737616777420044, 0.11447163671255112, 0.14357073605060577, 0.13235485553741455, 0.1219698116183281, 0.12082193791866302, 0.1261785328388214, 0.11959898471832275, 0.12303849309682846, 0.11966122686862946, 0.11029355227947235, 0.12251894176006317, 0.1281532347202301, 0.11705019325017929, 0.11349010467529297, 0.09656228125095367, 0.0940602570772171, 0.10487449169158936, 0.1457107812166214, 0.16357135772705078, 0.1685556173324585, 0.14184598624706268, 0.13357719779014587, 0.1120605617761612, 0.11645755171775818, 0.11830711364746094, 0.12925656139850616, 0.10890086740255356, 0.1136988028883934, 0.10902911424636841, 0.13088811933994293, 0.12327509373426437, 0.120126873254776, 0.11476263403892517, 0.12207897752523422, 0.13805493712425232, 0.15938474237918854, 0.15672516822814941, 0.17583230137825012, 0.1477123349905014, 0.1269867718219757, 0.10258522629737854, 0.1084737554192543, 0.10384342074394226, 0.10164468735456467, 0.11524346470832825, 0.1217750608921051, 0.12030037492513657, 0.13643759489059448, 0.15234386920928955, 0.15101653337478638, 0.13192087411880493, 0.11052052676677704, 0.08923447132110596, 0.08992525935173035, 0.1271190494298935, 0.16568978130817413, 0.15538334846496582, 0.1353185474872589, 0.13351094722747803, 0.12432311475276947, 0.12815901637077332, 0.12362492084503174, 0.14871340990066528, 0.14738020300865173, 0.1583077609539032, 0.16310790181159973, 0.15860819816589355, 0.13181059062480927, 0.1269245445728302, 0.1596602201461792, 0.137792706489563, 0.13851264119148254, 0.13580003380775452, 0.14584684371948242, 0.14112144708633423, 0.12793225049972534, 0.12753482162952423, 0.12532082200050354, 0.1254560649394989, 0.14687490463256836, 0.1435377299785614, 0.12214025855064392, 0.09906750917434692, 0.0943259745836258, 0.09347522258758545, 0.10634571313858032, 0.1145528256893158, 0.10692116618156433, 0.09991821646690369, 0.10688140988349915, 0.09634671360254288, 0.1149405986070633, 0.1469690501689911, 0.15783877670764923, 0.14909103512763977, 0.13287194073200226, 0.14271557331085205, 0.12938591837882996, 0.11645914614200592, 0.11804944276809692, 0.1430872678756714, 0.14054197072982788, 0.11711046099662781, 0.10741019248962402, 0.11023646593093872, 0.1065567210316658, 0.09098710119724274, 0.08613355457782745, 0.10665933787822723, 0.135981023311615, 0.133607879281044, 0.15591159462928772, 0.16582167148590088, 0.14037472009658813, 0.11786657571792603, 0.1313508152961731, 0.13011324405670166, 0.11807289719581604, 0.11539243161678314, 0.13395065069198608, 0.12802432477474213, 0.13531938195228577, 0.1393643021583557, 0.10813742876052856, 0.08920195698738098, 0.08316103368997574, 0.11782185733318329, 0.12405939400196075, 0.12154512852430344, 0.11340297758579254, 0.11551826447248459, 0.12837445735931396, 0.16914662718772888, 0.18031473457813263, 0.12255489826202393, 0.11479540914297104, 0.13220807909965515, 0.14713625609874725, 0.1368822455406189, 0.12468752264976501, 0.11083699762821198, 0.1385158747434616, 0.14815080165863037, 0.18413686752319336, 0.1825505793094635, 0.17830349504947662, 0.14800067245960236, 0.1547817438840866, 0.14019691944122314, 0.10511276125907898, 0.11664809286594391, 0.11712878942489624, 0.11722715198993683, 0.12089335918426514, 0.14046677947044373, 0.1467532068490982, 0.15148067474365234, 0.1329464614391327, 0.10089631378650665, 0.10620008409023285, 0.14466634392738342, 0.1421113759279251, 0.11816765367984772, 0.11810940504074097, 0.11442507058382034, 0.10744461417198181, 0.09907573461532593, 0.10483135282993317, 0.09771938621997833, 0.12347158789634705, 0.1488274484872818, 0.16033083200454712, 0.17676237225532532, 0.14292597770690918, 0.14010143280029297, 0.14245370030403137, 0.15829582512378693, 0.1389550268650055, 0.12319887429475784, 0.10312826931476593, 0.12551423907279968, 0.14777439832687378, 0.1368955373764038, 0.1381233036518097, 0.13355271518230438, 0.12458348274230957, 0.10648810863494873, 0.12186384201049805, 0.12996584177017212, 0.12637414038181305, 0.140517920255661, 0.13927021622657776, 0.12296129763126373, 0.1106024757027626, 0.0994834154844284, 0.1028374433517456, 0.09507807344198227, 0.11255353689193726, 0.11645198613405228, 0.12285979837179184, 0.11476181447505951, 0.11728449165821075, 0.09981599450111389, 0.08738194406032562, 0.1025468111038208, 0.11681543290615082, 0.13568046689033508, 0.1282796412706375, 0.14216919243335724, 0.16554957628250122, 0.1643090844154358, 0.16018515825271606, 0.14803661406040192, 0.14459171891212463, 0.10854504257440567, 0.1214919239282608, 0.14027658104896545, 0.14865627884864807, 0.12749506533145905, 0.1067066341638565, 0.09358534216880798, 0.09820082038640976, 0.11696112155914307, 0.11612170934677124, 0.11856038868427277, 0.1198955550789833, 0.13316082954406738, 0.11396795511245728, 0.10880971699953079, 0.1582396924495697, 0.15964378416538239, 0.12391526252031326, 0.11286565661430359, 0.12226836383342743, 0.1317858248949051, 0.1242949515581131, 0.12940910458564758, 0.1265377253293991, 0.10747764259576797, 0.08897317945957184, 0.08447594940662384, 0.10818521678447723, 0.12594975531101227, 0.1375441998243332, 0.12260220944881439, 0.12195175886154175, 0.10649731755256653, 0.0892457440495491, 0.08114758133888245, 0.10540696233510971, 0.13732171058654785, 0.14039096236228943, 0.1511535942554474, 0.1676928997039795, 0.14804132282733917, 0.12730082869529724, 0.10991707444190979, 0.11596854031085968, 0.09837540239095688, 0.12064246833324432, 0.15904387831687927, 0.1452815979719162, 0.11016080528497696, 0.15911313891410828, 0.175297349691391, 0.14917294681072235, 0.12446034699678421, 0.11348926275968552, 0.12412537634372711, 0.13892295956611633, 0.1252724826335907, 0.13037461042404175, 0.12475113570690155, 0.1342116743326187, 0.11634507775306702, 0.13246718049049377, 0.15343299508094788, 0.1572616994380951, 0.1546500325202942, 0.12907278537750244, 0.0937051773071289, 0.11493624746799469, 0.14529240131378174, 0.13731148838996887, 0.1057947427034378, 0.10757484287023544, 0.13161805272102356, 0.13272663950920105, 0.13064256310462952, 0.13051678240299225, 0.12647350132465363, 0.09653455018997192, 0.08986920118331909, 0.0921025276184082, 0.11002504080533981, 0.14039263129234314, 0.15716111660003662, 0.14067471027374268, 0.11592395603656769, 0.10043711215257645, 0.06900634616613388, 0.0739544928073883, 0.09229427576065063, 0.10478830337524414, 0.09628663212060928, 0.1005316823720932, 0.11508207023143768, 0.11960877478122711, 0.12745054066181183, 0.12211160361766815, 0.118714839220047, 0.12047991156578064, 0.11309154331684113, 0.10771432518959045, 0.0859108418226242, 0.10979476571083069, 0.11926784366369247, 0.12541137635707855, 0.12592431902885437, 0.11991536617279053, 0.09595450758934021, 0.0958976075053215, 0.10680058598518372, 0.12302446365356445, 0.14519649744033813, 0.1398952454328537, 0.1307833194732666, 0.15214401483535767, 0.14455705881118774, 0.12690909206867218, 0.10924969613552094, 0.11782634258270264, 0.1120690256357193, 0.11578743159770966, 0.1354002058506012, 0.14392542839050293, 0.15803007781505585, 0.16551148891448975, 0.15911823511123657, 0.13385827839374542, 0.1540912538766861, 0.16137616336345673, 0.14378023147583008, 0.11762065440416336, 0.14155805110931396, 0.17109829187393188, 0.1493857502937317, 0.140928253531456, 0.12154895067214966, 0.14389880001544952, 0.14229100942611694, 0.1428590714931488, 0.11483505368232727, 0.0965556800365448, 0.07490116357803345, 0.10696777701377869, 0.14244131743907928, 0.13411760330200195, 0.09845907986164093, 0.11153886467218399, 0.14761359989643097, 0.1688837707042694, 0.16010628640651703, 0.14501765370368958, 0.11155204474925995, 0.0939936637878418, 0.14028355479240417, 0.14834703505039215, 0.12729544937610626, 0.11955401301383972, 0.1368243396282196, 0.13119369745254517, 0.1349847912788391, 0.14419206976890564, 0.1497282087802887, 0.13483695685863495, 0.11643841862678528, 0.10256943106651306, 0.1050800234079361, 0.1589905470609665, 0.16761580109596252, 0.1475752592086792, 0.11638031899929047, 0.12588998675346375, 0.12799274921417236, 0.12204441428184509, 0.11290466785430908, 0.12504743039608002, 0.1289672553539276, 0.14211556315422058, 0.1387520134449005, 0.13823160529136658, 0.09993385523557663, 0.10355609655380249, 0.1078166514635086, 0.11356622725725174, 0.11403064429759979, 0.11733411252498627, 0.13665562868118286, 0.13691715896129608, 0.14489245414733887, 0.1334206461906433, 0.12302003055810928, 0.10969473421573639, 0.09805820882320404, 0.10649989545345306, 0.1383109986782074, 0.14458127319812775, 0.1230187863111496, 0.1164480596780777, 0.14025455713272095, 0.15444836020469666, 0.13973325490951538, 0.1439492404460907, 0.13349804282188416, 0.10830001533031464, 0.10331913828849792, 0.10580753535032272, 0.10478825867176056, 0.11652684211730957, 0.14632554352283478, 0.128605917096138, 0.1020885482430458, 0.10829295963048935, 0.10254915058612823, 0.11458584666252136, 0.11766146123409271, 0.09479647129774094, 0.1135343611240387, 0.1486751139163971, 0.15488573908805847, 0.13097941875457764, 0.11972729861736298, 0.123055599629879, 0.11650703847408295, 0.14850738644599915, 0.1566908061504364, 0.13818125426769257, 0.12238845974206924, 0.10561954975128174, 0.10562033206224442, 0.10706663131713867, 0.12792536616325378, 0.1271275281906128, 0.14377157390117645, 0.15980422496795654, 0.1467243880033493, 0.15188884735107422, 0.1177918091416359, 0.1521807610988617, 0.14611533284187317, 0.13064119219779968, 0.13106362521648407, 0.12039357423782349, 0.0999593660235405, 0.16563722491264343, 0.18687212467193604, 0.16721200942993164, 0.12712131440639496, 0.11982996761798859, 0.10831112414598465, 0.11500278860330582, 0.12532943487167358, 0.14523980021476746, 0.15220993757247925, 0.1476251482963562, 0.12438076734542847, 0.10536784678697586, 0.0962945967912674, 0.10795404016971588, 0.11994699388742447, 0.12542188167572021, 0.1437070071697235, 0.1351228952407837, 0.1223873496055603, 0.10702958703041077, 0.11430267244577408, 0.1205802857875824, 0.12184011936187744, 0.1239069476723671, 0.13420267403125763, 0.12466555088758469, 0.1313961297273636, 0.15742459893226624, 0.14156073331832886, 0.10211960226297379, 0.08575961738824844, 0.09203116595745087, 0.11712820082902908, 0.13829702138900757, 0.1339586079120636, 0.1299332082271576, 0.12835541367530823, 0.12282535433769226, 0.10937616229057312, 0.11687498539686203, 0.10064047574996948, 0.10154922306537628, 0.1037115827202797, 0.15571023523807526, 0.17933285236358643, 0.173940971493721, 0.16292783617973328, 0.14333570003509521, 0.12108196318149567, 0.10831446945667267, 0.10774920135736465, 0.11688896268606186, 0.1207507997751236, 0.11613737046718597, 0.12224115431308746, 0.11663424223661423, 0.1549576371908188, 0.14640803635120392, 0.1342274695634842, 0.11893144994974136, 0.15973883867263794, 0.16149136424064636, 0.18374931812286377, 0.1777087301015854, 0.16085559129714966, 0.14234647154808044, 0.17607611417770386, 0.13277502357959747, 0.10847028344869614, 0.11501283943653107, 0.13659818470478058, 0.15140032768249512, 0.1706317663192749, 0.16619467735290527, 0.15451686084270477, 0.12894120812416077, 0.12984465062618256, 0.139362633228302, 0.12421824038028717, 0.0916382372379303, 0.08828392624855042, 0.11417020857334137, 0.11285798996686935, 0.11434536427259445, 0.09005925059318542, 0.10598341375589371, 0.12984664738178253, 0.14410914480686188, 0.15133881568908691, 0.12900714576244354, 0.1347077488899231, 0.13213469088077545, 0.11885565519332886, 0.11758700013160706, 0.12045514583587646, 0.13353872299194336, 0.1379452794790268, 0.13201922178268433, 0.11247190833091736, 0.13030549883842468, 0.13065896928310394, 0.13856518268585205, 0.13732841610908508, 0.13612569868564606, 0.13287654519081116, 0.11623004078865051, 0.10300132632255554, 0.1024838387966156, 0.11934369057416916, 0.13613706827163696, 0.1242007240653038, 0.1128375381231308, 0.14200253784656525, 0.18235602974891663, 0.17403849959373474, 0.16088631749153137, 0.1566009670495987, 0.14677156507968903, 0.11444836854934692, 0.09210989624261856, 0.08341404795646667, 0.07827979326248169, 0.09688490629196167, 0.10619715601205826, 0.093761146068573, 0.09328807890415192, 0.0930321216583252, 0.1118476614356041, 0.11449176073074341, 0.13980209827423096, 0.13898643851280212, 0.15145310759544373, 0.14044028520584106, 0.11991855502128601, 0.13450491428375244, 0.14306865632534027, 0.14089782536029816, 0.14458151161670685, 0.15569068491458893, 0.16849935054779053, 0.18595990538597107, 0.15180134773254395, 0.14350208640098572, 0.13495802879333496, 0.15639473497867584, 0.11651875078678131, 0.0807429701089859, 0.10184387117624283, 0.137518972158432, 0.19485563039779663, 0.20096445083618164, 0.1898474544286728, 0.1894085705280304, 0.1479860246181488, 0.10378623008728027, 0.11834047734737396, 0.13786299526691437, 0.19476884603500366, 0.20104077458381653, 0.1775721311569214, 0.1446390002965927, 0.15050959587097168, 0.13324114680290222, 0.09641201794147491, 0.09860917925834656, 0.14135922491550446, 0.1509784609079361, 0.14532330632209778, 0.11472391337156296, 0.16883325576782227, 0.15197665989398956, 0.13080506026744843, 0.13163304328918457, 0.13626304268836975, 0.1336248219013214, 0.14963185787200928, 0.15388083457946777, 0.15109148621559143, 0.14284732937812805, 0.1277906894683838, 0.10725834965705872, 0.12609577178955078, 0.14936327934265137, 0.13506509363651276, 0.13956543803215027, 0.14237400889396667, 0.1647772490978241, 0.14574919641017914, 0.10313980281352997, 0.08347364515066147, 0.07528766244649887, 0.06683381646871567, 0.0547374002635479, 0.06394411623477936, 0.0928831696510315, 0.13552415370941162, 0.1260526031255722, 0.139265775680542, 0.17061178386211395, 0.17069891095161438, 0.1997072845697403, 0.22111786901950836, 0.21559883654117584, 0.1701330989599228, 0.1495913565158844, 0.13573762774467468, 0.1420125961303711, 0.1581500768661499, 0.15499556064605713, 0.1522173285484314, 0.1262671798467636, 0.12050884962081909, 0.12372042238712311, 0.11953148990869522, 0.08881726861000061, 0.08480103313922882, 0.09380150586366653, 0.10961699485778809, 0.10666172206401825, 0.11679089069366455, 0.1302785873413086, 0.11099697649478912, 0.12057919800281525, 0.10667631775140762, 0.09605467319488525, 0.1125592365860939, 0.11663837730884552, 0.10329153388738632, 0.11114561557769775, 0.12225326150655746, 0.1391376256942749, 0.1321536898612976, 0.10947291553020477, 0.10499849915504456, 0.1362840235233307, 0.1456506848335266, 0.11870124936103821, 0.12013359367847443, 0.09982700645923615, 0.11673997342586517, 0.10364019125699997, 0.13920733332633972, 0.14665274322032928, 0.15944430232048035, 0.15037092566490173, 0.1330515444278717, 0.1137537881731987, 0.11524400115013123, 0.13266649842262268, 0.14024481177330017, 0.1555543839931488, 0.16642248630523682, 0.14493775367736816, 0.11908066272735596, 0.1078222468495369, 0.11659950762987137, 0.12816902995109558, 0.1228434294462204, 0.10381873697042465, 0.0972185730934143, 0.10661265254020691, 0.09261766076087952, 0.09667189419269562, 0.10212239623069763, 0.1112104132771492, 0.10923363268375397, 0.12385791540145874, 0.14764603972434998, 0.15689659118652344, 0.17509371042251587, 0.14737898111343384, 0.14081043004989624, 0.13253791630268097, 0.13239067792892456, 0.15132129192352295, 0.1296217143535614, 0.12962007522583008, 0.11904831975698471, 0.12987345457077026, 0.1403883993625641, 0.1526767909526825, 0.1424250453710556, 0.13401930034160614, 0.09462863206863403, 0.10354644060134888, 0.08929652720689774, 0.12842395901679993, 0.16117019951343536, 0.17416976392269135, 0.13222146034240723, 0.08252417296171188, 0.08359389752149582, 0.09734585136175156, 0.12060405313968658, 0.13741087913513184, 0.13586026430130005, 0.11711526662111282, 0.11172541230916977, 0.14710292220115662, 0.13737446069717407, 0.11504055559635162, 0.10845828056335449, 0.14841187000274658, 0.1806512475013733, 0.1606954038143158, 0.16714951395988464, 0.15379145741462708, 0.13242527842521667, 0.09630594402551651, 0.10666999220848083, 0.1411972939968109, 0.14559796452522278, 0.1163739264011383, 0.12649306654930115, 0.12244641035795212, 0.11059744656085968, 0.12548430263996124, 0.11112472414970398, 0.11070361733436584, 0.12082471698522568, 0.1519586741924286, 0.1540607064962387, 0.13793037831783295, 0.11276683956384659, 0.10649782419204712, 0.09033259749412537, 0.10660190880298615, 0.1479015052318573, 0.1424751579761505, 0.11865375190973282, 0.11929312348365784, 0.1249840036034584, 0.13737577199935913, 0.14874902367591858, 0.14085951447486877, 0.12024898827075958, 0.12920862436294556, 0.1259298324584961, 0.11968865245580673, 0.11828678101301193, 0.10690994560718536, 0.10457172989845276, 0.10592731833457947, 0.11020714789628983, 0.11858727037906647, 0.13528963923454285, 0.13303552567958832, 0.1294417828321457, 0.11561211198568344, 0.1339626908302307, 0.1344597339630127, 0.1552405059337616, 0.17210467159748077, 0.17681235074996948, 0.19403724372386932, 0.17677801847457886, 0.13685235381126404, 0.09061866998672485, 0.11284759640693665, 0.14270910620689392, 0.16820776462554932, 0.17425072193145752, 0.14300134778022766, 0.13294778764247894, 0.12806867063045502, 0.13164415955543518, 0.1348767727613449, 0.14535140991210938, 0.1613404005765915, 0.186731219291687, 0.1682388335466385, 0.131286159157753, 0.1375148892402649, 0.14666999876499176, 0.15107358992099762, 0.13129249215126038, 0.10714185982942581, 0.10080093890428543, 0.12433138489723206, 0.15325331687927246, 0.13150152564048767, 0.13070815801620483, 0.1283664107322693, 0.13558214902877808, 0.11640458554029465, 0.13565361499786377, 0.15521594882011414, 0.15173931419849396, 0.1321452260017395, 0.10180425643920898, 0.11241619288921356]\n",
            "output yArray Shape:  1055\n",
            "<Figure size 640x480 with 1 Axes>\n"
          ]
        }
      ],
      "source": [
        "!python inference.py -h5Dataset \"SampleVideo.h5\" -video_name \"JungleVideo\" -model_weight \"model_weights/summe_random_non_overlap_0.5359.tar.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSevCfGtQAFy",
        "outputId": "1ce02598-a64d-4cfa-8a6c-d59c92eb4833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1055\n",
            "[1, 2, 3, 4, 5, 6, 16, 30, 31, 93, 94, 109, 110, 120, 121, 140, 150, 151, 154, 155, 156, 198, 203, 204, 226, 227, 243, 245, 263, 273, 277, 320, 340, 341, 351, 352, 353, 379, 380, 417, 418, 419, 438, 461, 471, 472, 545, 546, 549, 553, 569, 570, 588, 653, 663, 664, 665, 708, 709, 710, 724, 725, 726, 727, 728, 730, 736, 737, 777, 778, 779, 803, 804, 813, 814, 815, 816, 821, 822, 823, 833, 850, 862, 863, 864, 865, 866, 867, 907, 915, 933, 951, 952, 967, 968, 969, 1016, 1017, 1018, 1019, 1024, 1025, 1032, 1033, 1034]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "chunk:  73%|███████▎  | 1020/1390 [20:17<00:01, 285.04it/s, now=None]\n",
            "chunk:   0%|          | 0/2316 [00:00<?, ?it/s, now=None]\u001b[A\n",
            "chunk:   3%|▎         | 66/2316 [00:00<00:03, 655.83it/s, now=None]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video Summary/JungleVideo.mp4.\n",
            "MoviePy - Writing audio in JungleVideoTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "chunk:   6%|▌         | 132/2316 [00:00<00:03, 607.24it/s, now=None]\u001b[A\n",
            "chunk:   8%|▊         | 193/2316 [00:00<00:07, 291.39it/s, now=None]\u001b[A\n",
            "chunk:  10%|█         | 235/2316 [00:00<00:09, 216.34it/s, now=None]\u001b[A\n",
            "chunk:  11%|█▏        | 266/2316 [00:01<00:11, 177.39it/s, now=None]\u001b[A\n",
            "chunk:  13%|█▎        | 290/2316 [00:01<00:11, 172.35it/s, now=None]\u001b[A\n",
            "chunk:  13%|█▎        | 311/2316 [00:01<00:12, 166.16it/s, now=None]\u001b[A\n",
            "chunk:  14%|█▍        | 332/2316 [00:01<00:12, 159.52it/s, now=None]\u001b[A\n",
            "chunk:  15%|█▌        | 354/2316 [00:01<00:12, 154.92it/s, now=None]\u001b[A\n",
            "chunk:  16%|█▌        | 376/2316 [00:01<00:13, 148.28it/s, now=None]\u001b[A\n",
            "chunk:  17%|█▋        | 399/2316 [00:02<00:12, 149.79it/s, now=None]\u001b[A\n",
            "chunk:  18%|█▊        | 421/2316 [00:02<00:13, 145.39it/s, now=None]\u001b[A\n",
            "chunk:  19%|█▉        | 443/2316 [00:02<00:12, 145.18it/s, now=None]\u001b[A\n",
            "chunk:  20%|██        | 465/2316 [00:02<00:12, 143.24it/s, now=None]\u001b[A\n",
            "chunk:  21%|██        | 487/2316 [00:02<00:12, 144.23it/s, now=None]\u001b[A\n",
            "chunk:  22%|██▏       | 509/2316 [00:02<00:12, 144.26it/s, now=None]\u001b[A\n",
            "chunk:  23%|██▎       | 531/2316 [00:02<00:12, 144.10it/s, now=None]\u001b[A\n",
            "chunk:  24%|██▍       | 553/2316 [00:03<00:12, 146.26it/s, now=None]\u001b[A\n",
            "chunk:  25%|██▍       | 575/2316 [00:03<00:12, 144.11it/s, now=None]\u001b[A\n",
            "chunk:  26%|██▌       | 597/2316 [00:03<00:11, 145.22it/s, now=None]\u001b[A\n",
            "chunk:  27%|██▋       | 619/2316 [00:03<00:11, 145.96it/s, now=None]\u001b[A\n",
            "chunk:  28%|██▊       | 641/2316 [00:03<00:11, 144.44it/s, now=None]\u001b[A\n",
            "chunk:  29%|██▊       | 663/2316 [00:03<00:11, 146.46it/s, now=None]\u001b[A\n",
            "chunk:  30%|██▉       | 685/2316 [00:04<00:11, 146.29it/s, now=None]\u001b[A\n",
            "chunk:  31%|███       | 707/2316 [00:04<00:11, 144.14it/s, now=None]\u001b[A\n",
            "chunk:  31%|███▏      | 729/2316 [00:04<00:11, 138.48it/s, now=None]\u001b[A\n",
            "chunk:  32%|███▏      | 751/2316 [00:04<00:11, 139.91it/s, now=None]\u001b[A\n",
            "chunk:  33%|███▎      | 774/2316 [00:04<00:10, 140.26it/s, now=None]\u001b[A\n",
            "chunk:  34%|███▍      | 796/2316 [00:04<00:10, 139.50it/s, now=None]\u001b[A\n",
            "chunk:  35%|███▌      | 818/2316 [00:05<00:10, 140.16it/s, now=None]\u001b[A\n",
            "chunk:  36%|███▋      | 840/2316 [00:05<00:10, 139.98it/s, now=None]\u001b[A\n",
            "chunk:  37%|███▋      | 862/2316 [00:05<00:10, 136.58it/s, now=None]\u001b[A\n",
            "chunk:  38%|███▊      | 884/2316 [00:05<00:10, 136.33it/s, now=None]\u001b[A\n",
            "chunk:  39%|███▉      | 906/2316 [00:05<00:10, 137.88it/s, now=None]\u001b[A\n",
            "chunk:  40%|████      | 928/2316 [00:05<00:10, 135.89it/s, now=None]\u001b[A\n",
            "chunk:  41%|████      | 950/2316 [00:05<00:09, 137.53it/s, now=None]\u001b[A\n",
            "chunk:  42%|████▏     | 972/2316 [00:06<00:09, 141.32it/s, now=None]\u001b[A\n",
            "chunk:  43%|████▎     | 994/2316 [00:06<00:09, 143.28it/s, now=None]\u001b[A\n",
            "chunk:  44%|████▍     | 1016/2316 [00:06<00:09, 143.17it/s, now=None]\u001b[A\n",
            "chunk:  45%|████▍     | 1038/2316 [00:06<00:08, 144.27it/s, now=None]\u001b[A\n",
            "chunk:  46%|████▌     | 1060/2316 [00:06<00:08, 145.50it/s, now=None]\u001b[A\n",
            "chunk:  47%|████▋     | 1082/2316 [00:06<00:08, 145.48it/s, now=None]\u001b[A\n",
            "chunk:  48%|████▊     | 1104/2316 [00:07<00:08, 142.88it/s, now=None]\u001b[A\n",
            "chunk:  49%|████▊     | 1126/2316 [00:07<00:08, 144.37it/s, now=None]\u001b[A\n",
            "chunk:  50%|████▉     | 1148/2316 [00:07<00:08, 143.69it/s, now=None]\u001b[A\n",
            "chunk:  51%|█████     | 1171/2316 [00:07<00:07, 146.42it/s, now=None]\u001b[A\n",
            "chunk:  52%|█████▏    | 1193/2316 [00:07<00:07, 142.49it/s, now=None]\u001b[A\n",
            "chunk:  52%|█████▏    | 1215/2316 [00:07<00:07, 142.56it/s, now=None]\u001b[A\n",
            "chunk:  53%|█████▎    | 1237/2316 [00:07<00:07, 143.96it/s, now=None]\u001b[A\n",
            "chunk:  54%|█████▍    | 1259/2316 [00:08<00:07, 144.20it/s, now=None]\u001b[A\n",
            "chunk:  55%|█████▌    | 1281/2316 [00:08<00:07, 144.99it/s, now=None]\u001b[A\n",
            "chunk:  56%|█████▋    | 1303/2316 [00:08<00:07, 143.43it/s, now=None]\u001b[A\n",
            "chunk:  57%|█████▋    | 1325/2316 [00:08<00:06, 143.57it/s, now=None]\u001b[A\n",
            "chunk:  58%|█████▊    | 1347/2316 [00:08<00:07, 138.32it/s, now=None]\u001b[A\n",
            "chunk:  59%|█████▉    | 1369/2316 [00:08<00:06, 137.85it/s, now=None]\u001b[A\n",
            "chunk:  60%|██████    | 1391/2316 [00:09<00:06, 139.14it/s, now=None]\u001b[A\n",
            "chunk:  61%|██████    | 1413/2316 [00:09<00:06, 141.64it/s, now=None]\u001b[A\n",
            "chunk:  62%|██████▏   | 1435/2316 [00:09<00:06, 142.78it/s, now=None]\u001b[A\n",
            "chunk:  63%|██████▎   | 1457/2316 [00:09<00:06, 141.95it/s, now=None]\u001b[A\n",
            "chunk:  64%|██████▍   | 1479/2316 [00:09<00:05, 142.08it/s, now=None]\u001b[A\n",
            "chunk:  65%|██████▍   | 1501/2316 [00:09<00:05, 140.69it/s, now=None]\u001b[A\n",
            "chunk:  66%|██████▌   | 1523/2316 [00:09<00:05, 143.30it/s, now=None]\u001b[A\n",
            "chunk:  67%|██████▋   | 1546/2316 [00:10<00:05, 143.24it/s, now=None]\u001b[A\n",
            "chunk:  68%|██████▊   | 1568/2316 [00:10<00:05, 144.06it/s, now=None]\u001b[A\n",
            "chunk:  69%|██████▊   | 1590/2316 [00:10<00:04, 145.58it/s, now=None]\u001b[A\n",
            "chunk:  70%|██████▉   | 1612/2316 [00:10<00:04, 144.68it/s, now=None]\u001b[A\n",
            "chunk:  71%|███████   | 1634/2316 [00:10<00:04, 142.04it/s, now=None]\u001b[A\n",
            "chunk:  72%|███████▏  | 1656/2316 [00:10<00:04, 141.13it/s, now=None]\u001b[A\n",
            "chunk:  72%|███████▏  | 1678/2316 [00:11<00:04, 136.71it/s, now=None]\u001b[A\n",
            "chunk:  73%|███████▎  | 1700/2316 [00:11<00:04, 138.52it/s, now=None]\u001b[A\n",
            "chunk:  74%|███████▍  | 1722/2316 [00:11<00:04, 136.73it/s, now=None]\u001b[A\n",
            "chunk:  75%|███████▌  | 1744/2316 [00:11<00:04, 138.87it/s, now=None]\u001b[A\n",
            "chunk:  76%|███████▋  | 1766/2316 [00:11<00:03, 137.66it/s, now=None]\u001b[A\n",
            "chunk:  77%|███████▋  | 1788/2316 [00:11<00:03, 140.21it/s, now=None]\u001b[A\n",
            "chunk:  78%|███████▊  | 1810/2316 [00:12<00:03, 135.56it/s, now=None]\u001b[A\n",
            "chunk:  79%|███████▉  | 1832/2316 [00:12<00:03, 136.50it/s, now=None]\u001b[A\n",
            "chunk:  80%|████████  | 1854/2316 [00:12<00:03, 139.22it/s, now=None]\u001b[A\n",
            "chunk:  81%|████████  | 1876/2316 [00:12<00:03, 141.41it/s, now=None]\u001b[A\n",
            "chunk:  82%|████████▏ | 1898/2316 [00:12<00:02, 140.06it/s, now=None]\u001b[A\n",
            "chunk:  83%|████████▎ | 1920/2316 [00:12<00:02, 142.87it/s, now=None]\u001b[A\n",
            "chunk:  84%|████████▍ | 1943/2316 [00:12<00:02, 142.48it/s, now=None]\u001b[A\n",
            "chunk:  85%|████████▍ | 1965/2316 [00:13<00:02, 140.84it/s, now=None]\u001b[A\n",
            "chunk:  86%|████████▌ | 1987/2316 [00:13<00:02, 137.26it/s, now=None]\u001b[A\n",
            "chunk:  87%|████████▋ | 2009/2316 [00:13<00:02, 140.58it/s, now=None]\u001b[A\n",
            "chunk:  88%|████████▊ | 2031/2316 [00:13<00:02, 139.65it/s, now=None]\u001b[A\n",
            "chunk:  89%|████████▊ | 2053/2316 [00:13<00:01, 139.95it/s, now=None]\u001b[A\n",
            "chunk:  90%|████████▉ | 2075/2316 [00:13<00:01, 140.33it/s, now=None]\u001b[A\n",
            "chunk:  91%|█████████ | 2097/2316 [00:14<00:01, 141.87it/s, now=None]\u001b[A\n",
            "chunk:  91%|█████████▏| 2119/2316 [00:14<00:01, 143.16it/s, now=None]\u001b[A\n",
            "chunk:  92%|█████████▏| 2141/2316 [00:14<00:01, 142.49it/s, now=None]\u001b[A\n",
            "chunk:  93%|█████████▎| 2163/2316 [00:14<00:01, 141.00it/s, now=None]\u001b[A\n",
            "chunk:  94%|█████████▍| 2185/2316 [00:14<00:00, 141.13it/s, now=None]\u001b[A\n",
            "chunk:  95%|█████████▌| 2207/2316 [00:14<00:00, 139.34it/s, now=None]\u001b[A\n",
            "chunk:  96%|█████████▌| 2229/2316 [00:15<00:00, 140.72it/s, now=None]\u001b[A\n",
            "chunk:  97%|█████████▋| 2251/2316 [00:15<00:00, 141.83it/s, now=None]\u001b[A\n",
            "chunk:  98%|█████████▊| 2273/2316 [00:15<00:00, 142.57it/s, now=None]\u001b[A\n",
            "chunk:  99%|█████████▉| 2295/2316 [00:15<00:00, 141.97it/s, now=None]\u001b[A\n",
            "chunk:  73%|███████▎  | 1020/1390 [20:33<00:01, 285.04it/s, now=None]\n",
            "t:   0%|          | 0/3147 [00:00<?, ?it/s, now=None]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video Summary/JungleVideo.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "t:   0%|          | 6/3147 [00:00<01:09, 45.07it/s, now=None]\u001b[A\n",
            "t:   1%|          | 24/3147 [00:00<00:28, 111.35it/s, now=None]\u001b[A\n",
            "t:   1%|          | 38/3147 [00:00<00:25, 120.41it/s, now=None]\u001b[A\n",
            "t:   2%|▏         | 52/3147 [00:00<00:24, 124.83it/s, now=None]\u001b[A\n",
            "t:   2%|▏         | 65/3147 [00:00<00:49, 62.25it/s, now=None] \u001b[A\n",
            "t:   2%|▏         | 75/3147 [00:01<00:55, 55.33it/s, now=None]\u001b[A\n",
            "t:   3%|▎         | 83/3147 [00:01<00:58, 52.38it/s, now=None]\u001b[A\n",
            "t:   3%|▎         | 90/3147 [00:01<01:06, 45.83it/s, now=None]\u001b[A\n",
            "t:   3%|▎         | 96/3147 [00:01<01:08, 44.24it/s, now=None]\u001b[A\n",
            "t:   3%|▎         | 102/3147 [00:01<01:07, 45.39it/s, now=None]\u001b[A\n",
            "t:   3%|▎         | 109/3147 [00:01<01:04, 46.85it/s, now=None]\u001b[A\n",
            "t:   4%|▎         | 115/3147 [00:02<01:06, 45.63it/s, now=None]\u001b[A\n",
            "t:   4%|▍         | 121/3147 [00:02<01:04, 47.28it/s, now=None]\u001b[A\n",
            "t:   4%|▍         | 126/3147 [00:02<01:08, 43.87it/s, now=None]\u001b[A\n",
            "t:   4%|▍         | 133/3147 [00:02<01:03, 47.61it/s, now=None]\u001b[A\n",
            "t:   4%|▍         | 139/3147 [00:02<01:00, 49.86it/s, now=None]\u001b[A\n",
            "t:   5%|▍         | 145/3147 [00:02<01:12, 41.29it/s, now=None]\u001b[A\n",
            "t:   5%|▍         | 152/3147 [00:02<01:03, 47.33it/s, now=None]\u001b[A\n",
            "t:   5%|▌         | 158/3147 [00:02<01:01, 48.60it/s, now=None]\u001b[A\n",
            "t:   5%|▌         | 165/3147 [00:03<00:58, 51.06it/s, now=None]\u001b[A\n",
            "t:   5%|▌         | 171/3147 [00:03<00:56, 52.94it/s, now=None]\u001b[A\n",
            "t:   6%|▌         | 177/3147 [00:03<00:56, 52.75it/s, now=None]\u001b[A\n",
            "t:   6%|▌         | 183/3147 [00:03<00:57, 51.55it/s, now=None]\u001b[A\n",
            "t:   6%|▌         | 189/3147 [00:03<00:55, 53.54it/s, now=None]\u001b[A\n",
            "t:   6%|▌         | 195/3147 [00:03<01:04, 45.71it/s, now=None]\u001b[A\n",
            "t:   6%|▋         | 201/3147 [00:03<01:00, 48.36it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 207/3147 [00:03<00:58, 50.41it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 213/3147 [00:04<01:03, 46.00it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 218/3147 [00:04<01:06, 43.95it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 224/3147 [00:04<01:04, 45.59it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 229/3147 [00:04<01:04, 45.46it/s, now=None]\u001b[A\n",
            "t:   7%|▋         | 234/3147 [00:04<01:12, 40.20it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 240/3147 [00:04<01:20, 36.18it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 246/3147 [00:04<01:19, 36.39it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 251/3147 [00:05<01:13, 39.15it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 256/3147 [00:05<01:21, 35.62it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 260/3147 [00:05<01:24, 34.37it/s, now=None]\u001b[A\n",
            "t:   8%|▊         | 264/3147 [00:05<01:26, 33.36it/s, now=None]\u001b[A\n",
            "t:   9%|▊         | 269/3147 [00:05<01:27, 32.75it/s, now=None]\u001b[A\n",
            "t:   9%|▊         | 275/3147 [00:05<01:15, 37.98it/s, now=None]\u001b[A\n",
            "t:   9%|▉         | 279/3147 [00:05<01:19, 36.03it/s, now=None]\u001b[A\n",
            "t:   9%|▉         | 283/3147 [00:05<01:21, 35.05it/s, now=None]\u001b[A\n",
            "t:   9%|▉         | 289/3147 [00:06<01:11, 39.94it/s, now=None]\u001b[A\n",
            "t:   9%|▉         | 294/3147 [00:06<01:17, 36.70it/s, now=None]\u001b[A\n",
            "t:  10%|▉         | 300/3147 [00:06<01:14, 38.23it/s, now=None]\u001b[A\n",
            "t:  10%|▉         | 307/3147 [00:06<01:07, 42.39it/s, now=None]\u001b[A\n",
            "t:  10%|▉         | 313/3147 [00:06<01:02, 45.17it/s, now=None]\u001b[A\n",
            "t:  10%|█         | 318/3147 [00:06<01:01, 46.08it/s, now=None]\u001b[A\n",
            "t:  10%|█         | 324/3147 [00:06<00:57, 49.34it/s, now=None]\u001b[A\n",
            "t:  10%|█         | 330/3147 [00:07<01:04, 43.72it/s, now=None]\u001b[A\n",
            "t:  11%|█         | 335/3147 [00:07<01:02, 44.90it/s, now=None]\u001b[A\n",
            "t:  11%|█         | 340/3147 [00:07<01:04, 43.50it/s, now=None]\u001b[A\n",
            "t:  11%|█         | 345/3147 [00:07<01:06, 41.95it/s, now=None]\u001b[A\n",
            "t:  11%|█         | 351/3147 [00:07<01:00, 46.48it/s, now=None]\u001b[A\n",
            "t:  11%|█▏        | 358/3147 [00:07<00:55, 49.88it/s, now=None]\u001b[A\n",
            "t:  12%|█▏        | 365/3147 [00:07<00:50, 55.08it/s, now=None]\u001b[A\n",
            "t:  12%|█▏        | 374/3147 [00:07<00:44, 62.55it/s, now=None]\u001b[A\n",
            "t:  12%|█▏        | 383/3147 [00:07<00:39, 69.51it/s, now=None]\u001b[A\n",
            "t:  12%|█▏        | 391/3147 [00:08<00:39, 68.96it/s, now=None]\u001b[A\n",
            "t:  13%|█▎        | 398/3147 [00:08<00:49, 55.66it/s, now=None]\u001b[A\n",
            "t:  13%|█▎        | 405/3147 [00:08<00:48, 56.66it/s, now=None]\u001b[A\n",
            "t:  13%|█▎        | 411/3147 [00:08<00:49, 55.23it/s, now=None]\u001b[A\n",
            "t:  13%|█▎        | 417/3147 [00:08<00:48, 56.42it/s, now=None]\u001b[A\n",
            "t:  13%|█▎        | 423/3147 [00:08<00:58, 46.83it/s, now=None]\u001b[A\n",
            "t:  14%|█▎        | 429/3147 [00:08<01:00, 44.76it/s, now=None]\u001b[A\n",
            "t:  14%|█▍        | 435/3147 [00:09<00:57, 47.00it/s, now=None]\u001b[A\n",
            "t:  14%|█▍        | 441/3147 [00:09<00:56, 47.77it/s, now=None]\u001b[A\n",
            "t:  14%|█▍        | 446/3147 [00:09<01:02, 42.88it/s, now=None]\u001b[A\n",
            "t:  14%|█▍        | 451/3147 [00:09<01:14, 36.11it/s, now=None]\u001b[A\n",
            "t:  15%|█▍        | 458/3147 [00:09<01:08, 39.40it/s, now=None]\u001b[A\n",
            "t:  15%|█▍        | 464/3147 [00:09<01:04, 41.42it/s, now=None]\u001b[A\n",
            "t:  15%|█▍        | 469/3147 [00:09<01:06, 40.10it/s, now=None]\u001b[A\n",
            "t:  15%|█▌        | 474/3147 [00:10<01:07, 39.83it/s, now=None]\u001b[A\n",
            "t:  15%|█▌        | 479/3147 [00:10<01:07, 39.35it/s, now=None]\u001b[A\n",
            "t:  15%|█▌        | 485/3147 [00:10<01:00, 43.97it/s, now=None]\u001b[A\n",
            "t:  16%|█▌        | 490/3147 [00:10<01:04, 41.09it/s, now=None]\u001b[A\n",
            "t:  16%|█▌        | 495/3147 [00:10<01:11, 37.21it/s, now=None]\u001b[A\n",
            "t:  16%|█▌        | 500/3147 [00:10<01:06, 40.00it/s, now=None]\u001b[A\n",
            "t:  16%|█▌        | 505/3147 [00:10<01:08, 38.37it/s, now=None]\u001b[A\n",
            "t:  16%|█▌        | 510/3147 [00:10<01:09, 37.97it/s, now=None]\u001b[A\n",
            "t:  16%|█▋        | 516/3147 [00:11<01:01, 43.01it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 521/3147 [00:11<00:58, 44.65it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 526/3147 [00:11<00:58, 44.77it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 532/3147 [00:11<00:53, 48.63it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 538/3147 [00:11<00:51, 50.87it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 544/3147 [00:11<01:03, 41.04it/s, now=None]\u001b[A\n",
            "t:  17%|█▋        | 549/3147 [00:11<01:03, 41.12it/s, now=None]\u001b[A\n",
            "t:  18%|█▊        | 554/3147 [00:11<01:03, 40.89it/s, now=None]\u001b[A\n",
            "t:  18%|█▊        | 560/3147 [00:12<01:00, 42.82it/s, now=None]\u001b[A\n",
            "t:  18%|█▊        | 567/3147 [00:12<00:52, 49.49it/s, now=None]\u001b[A\n",
            "t:  18%|█▊        | 573/3147 [00:12<00:49, 51.98it/s, now=None]\u001b[A\n",
            "t:  18%|█▊        | 579/3147 [00:12<00:49, 51.40it/s, now=None]\u001b[A\n",
            "t:  19%|█▊        | 585/3147 [00:12<00:50, 50.75it/s, now=None]\u001b[A\n",
            "t:  19%|█▉        | 592/3147 [00:12<00:47, 54.31it/s, now=None]\u001b[A\n",
            "t:  19%|█▉        | 598/3147 [00:12<00:48, 52.70it/s, now=None]\u001b[A\n",
            "t:  19%|█▉        | 604/3147 [00:12<01:01, 41.46it/s, now=None]\u001b[A\n",
            "t:  19%|█▉        | 609/3147 [00:13<01:04, 39.16it/s, now=None]\u001b[A\n",
            "t:  20%|█▉        | 614/3147 [00:13<01:01, 41.40it/s, now=None]\u001b[A\n",
            "t:  20%|█▉        | 619/3147 [00:13<01:04, 39.00it/s, now=None]\u001b[A\n",
            "t:  20%|█▉        | 624/3147 [00:13<01:09, 36.31it/s, now=None]\u001b[A\n",
            "t:  20%|██        | 630/3147 [00:13<01:02, 40.10it/s, now=None]\u001b[A\n",
            "t:  20%|██        | 635/3147 [00:13<01:07, 36.96it/s, now=None]\u001b[A\n",
            "t:  20%|██        | 639/3147 [00:13<01:14, 33.49it/s, now=None]\u001b[A\n",
            "t:  20%|██        | 644/3147 [00:14<01:08, 36.70it/s, now=None]\u001b[A\n",
            "t:  21%|██        | 648/3147 [00:14<01:12, 34.56it/s, now=None]\u001b[A\n",
            "t:  21%|██        | 653/3147 [00:14<01:05, 38.04it/s, now=None]\u001b[A\n",
            "t:  21%|██        | 657/3147 [00:14<01:13, 33.66it/s, now=None]\u001b[A\n",
            "t:  21%|██        | 663/3147 [00:14<01:04, 38.44it/s, now=None]\u001b[A\n",
            "t:  21%|██        | 668/3147 [00:14<01:04, 38.55it/s, now=None]\u001b[A\n",
            "t:  21%|██▏       | 674/3147 [00:14<01:06, 37.40it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 679/3147 [00:14<01:01, 40.15it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 684/3147 [00:15<01:02, 39.22it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 689/3147 [00:15<01:06, 36.74it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 695/3147 [00:15<00:59, 41.55it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 700/3147 [00:15<01:05, 37.57it/s, now=None]\u001b[A\n",
            "t:  22%|██▏       | 704/3147 [00:15<01:05, 37.44it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 709/3147 [00:15<01:02, 38.89it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 713/3147 [00:15<01:07, 36.21it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 719/3147 [00:15<00:59, 40.68it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 724/3147 [00:16<01:07, 35.66it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 729/3147 [00:16<01:02, 38.76it/s, now=None]\u001b[A\n",
            "t:  23%|██▎       | 735/3147 [00:16<01:03, 38.20it/s, now=None]\u001b[A\n",
            "t:  24%|██▎       | 740/3147 [00:16<01:00, 39.64it/s, now=None]\u001b[A\n",
            "t:  24%|██▎       | 745/3147 [00:16<01:03, 37.62it/s, now=None]\u001b[A\n",
            "t:  24%|██▍       | 750/3147 [00:16<01:02, 38.53it/s, now=None]\u001b[A\n",
            "t:  24%|██▍       | 755/3147 [00:16<01:04, 37.37it/s, now=None]\u001b[A\n",
            "t:  24%|██▍       | 761/3147 [00:17<00:56, 41.96it/s, now=None]\u001b[A\n",
            "t:  24%|██▍       | 766/3147 [00:17<00:56, 42.46it/s, now=None]\u001b[A\n",
            "t:  24%|██▍       | 771/3147 [00:17<00:54, 43.35it/s, now=None]\u001b[A\n",
            "t:  25%|██▍       | 776/3147 [00:17<00:55, 42.51it/s, now=None]\u001b[A\n",
            "t:  25%|██▍       | 781/3147 [00:17<00:54, 43.14it/s, now=None]\u001b[A\n",
            "t:  25%|██▍       | 786/3147 [00:17<00:52, 44.94it/s, now=None]\u001b[A\n",
            "t:  25%|██▌       | 791/3147 [00:17<00:57, 41.14it/s, now=None]\u001b[A\n",
            "t:  25%|██▌       | 796/3147 [00:17<00:55, 41.99it/s, now=None]\u001b[A\n",
            "t:  25%|██▌       | 801/3147 [00:18<00:56, 41.38it/s, now=None]\u001b[A\n",
            "t:  26%|██▌       | 806/3147 [00:18<00:58, 39.80it/s, now=None]\u001b[A\n",
            "t:  26%|██▌       | 811/3147 [00:18<00:59, 39.23it/s, now=None]\u001b[A\n",
            "t:  26%|██▌       | 818/3147 [00:18<00:51, 45.47it/s, now=None]\u001b[A\n",
            "t:  26%|██▌       | 823/3147 [00:18<00:53, 43.59it/s, now=None]\u001b[A\n",
            "t:  26%|██▋       | 828/3147 [00:18<01:14, 31.02it/s, now=None]\u001b[A\n",
            "t:  26%|██▋       | 832/3147 [00:18<01:12, 31.76it/s, now=None]\u001b[A\n",
            "t:  27%|██▋       | 841/3147 [00:19<00:56, 41.00it/s, now=None]\u001b[A\n",
            "t:  27%|██▋       | 847/3147 [00:19<00:50, 45.21it/s, now=None]\u001b[A\n",
            "t:  27%|██▋       | 854/3147 [00:19<00:54, 41.91it/s, now=None]\u001b[A\n",
            "t:  27%|██▋       | 861/3147 [00:19<00:49, 46.03it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 866/3147 [00:19<00:55, 41.39it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 871/3147 [00:19<00:52, 43.31it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 877/3147 [00:19<00:51, 44.32it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 882/3147 [00:19<00:49, 45.47it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 887/3147 [00:20<00:59, 38.09it/s, now=None]\u001b[A\n",
            "t:  28%|██▊       | 893/3147 [00:20<01:00, 37.11it/s, now=None]\u001b[A\n",
            "t:  29%|██▊       | 898/3147 [00:20<00:57, 38.95it/s, now=None]\u001b[A\n",
            "t:  29%|██▊       | 903/3147 [00:20<01:05, 34.21it/s, now=None]\u001b[A\n",
            "t:  29%|██▉       | 908/3147 [00:20<01:10, 31.89it/s, now=None]\u001b[A\n",
            "t:  29%|██▉       | 915/3147 [00:20<00:56, 39.45it/s, now=None]\u001b[A\n",
            "t:  29%|██▉       | 920/3147 [00:21<00:57, 38.59it/s, now=None]\u001b[A\n",
            "t:  29%|██▉       | 926/3147 [00:21<00:53, 41.15it/s, now=None]\u001b[A\n",
            "t:  30%|██▉       | 931/3147 [00:21<01:02, 35.70it/s, now=None]\u001b[A\n",
            "t:  30%|██▉       | 936/3147 [00:21<01:02, 35.57it/s, now=None]\u001b[A\n",
            "t:  30%|██▉       | 944/3147 [00:21<00:55, 39.79it/s, now=None]\u001b[A\n",
            "t:  30%|███       | 950/3147 [00:21<00:49, 44.09it/s, now=None]\u001b[A\n",
            "t:  30%|███       | 955/3147 [00:21<00:56, 39.09it/s, now=None]\u001b[A\n",
            "t:  31%|███       | 960/3147 [00:22<01:03, 34.42it/s, now=None]\u001b[A\n",
            "t:  31%|███       | 965/3147 [00:22<00:58, 37.25it/s, now=None]\u001b[A\n",
            "t:  31%|███       | 969/3147 [00:22<01:05, 33.08it/s, now=None]\u001b[A\n",
            "t:  31%|███       | 975/3147 [00:22<01:02, 35.01it/s, now=None]\u001b[A\n",
            "t:  31%|███       | 979/3147 [00:22<01:09, 31.14it/s, now=None]\u001b[A\n",
            "t:  31%|███▏      | 985/3147 [00:22<01:03, 33.80it/s, now=None]\u001b[A\n",
            "t:  31%|███▏      | 989/3147 [00:22<01:03, 34.04it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 995/3147 [00:23<00:55, 38.61it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 999/3147 [00:23<00:57, 37.43it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 1003/3147 [00:23<01:08, 31.23it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 1010/3147 [00:23<00:56, 38.04it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 1015/3147 [00:23<00:55, 38.27it/s, now=None]\u001b[A\n",
            "t:  32%|███▏      | 1021/3147 [00:23<00:48, 43.45it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1026/3147 [00:23<00:55, 38.06it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1031/3147 [00:24<00:53, 39.74it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1037/3147 [00:24<00:48, 43.17it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1043/3147 [00:24<00:45, 46.69it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1048/3147 [00:24<00:50, 41.94it/s, now=None]\u001b[A\n",
            "t:  33%|███▎      | 1053/3147 [00:24<00:51, 40.94it/s, now=None]\u001b[A\n",
            "t:  34%|███▎      | 1058/3147 [00:24<00:59, 35.35it/s, now=None]\u001b[A\n",
            "t:  34%|███▎      | 1062/3147 [00:24<01:06, 31.15it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 1066/3147 [00:25<01:04, 32.37it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 1070/3147 [00:25<01:09, 30.10it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 1074/3147 [00:25<01:15, 27.30it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 1077/3147 [00:25<01:14, 27.73it/s, now=None]\u001b[A\n",
            "t:  34%|███▍      | 1080/3147 [00:25<01:17, 26.65it/s, now=None]\u001b[A\n",
            "t:  35%|███▍      | 1086/3147 [00:25<01:04, 31.71it/s, now=None]\u001b[A\n",
            "t:  35%|███▍      | 1092/3147 [00:25<00:54, 37.90it/s, now=None]\u001b[A\n",
            "t:  35%|███▍      | 1096/3147 [00:25<00:59, 34.66it/s, now=None]\u001b[A\n",
            "t:  35%|███▍      | 1100/3147 [00:26<01:00, 34.02it/s, now=None]\u001b[A\n",
            "t:  35%|███▌      | 1106/3147 [00:26<01:02, 32.57it/s, now=None]\u001b[A\n",
            "t:  35%|███▌      | 1112/3147 [00:26<00:54, 37.21it/s, now=None]\u001b[A\n",
            "t:  35%|███▌      | 1116/3147 [00:26<00:54, 37.35it/s, now=None]\u001b[A\n",
            "t:  36%|███▌      | 1120/3147 [00:26<00:59, 33.98it/s, now=None]\u001b[A\n",
            "t:  36%|███▌      | 1124/3147 [00:26<00:58, 34.78it/s, now=None]\u001b[A\n",
            "t:  36%|███▌      | 1128/3147 [00:26<00:59, 34.10it/s, now=None]\u001b[A\n",
            "t:  36%|███▌      | 1133/3147 [00:27<01:01, 32.63it/s, now=None]\u001b[A\n",
            "t:  36%|███▌      | 1137/3147 [00:27<00:58, 34.09it/s, now=None]\u001b[A\n",
            "t:  36%|███▋      | 1141/3147 [00:27<01:05, 30.82it/s, now=None]\u001b[A\n",
            "t:  36%|███▋      | 1146/3147 [00:27<00:57, 34.87it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1151/3147 [00:27<00:52, 38.19it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1155/3147 [00:27<00:54, 36.89it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1159/3147 [00:27<00:56, 35.18it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1163/3147 [00:27<00:58, 34.12it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1169/3147 [00:28<00:49, 39.59it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1174/3147 [00:28<00:52, 37.77it/s, now=None]\u001b[A\n",
            "t:  37%|███▋      | 1178/3147 [00:28<00:54, 36.14it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1182/3147 [00:28<00:55, 35.65it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1187/3147 [00:28<00:50, 38.44it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1191/3147 [00:28<00:50, 38.43it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1195/3147 [00:28<00:54, 35.93it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1201/3147 [00:28<00:50, 38.47it/s, now=None]\u001b[A\n",
            "t:  38%|███▊      | 1207/3147 [00:29<00:49, 39.23it/s, now=None]\u001b[A\n",
            "t:  39%|███▊      | 1214/3147 [00:29<00:47, 40.87it/s, now=None]\u001b[A\n",
            "t:  39%|███▉      | 1220/3147 [00:29<00:53, 35.94it/s, now=None]\u001b[A\n",
            "t:  39%|███▉      | 1224/3147 [00:29<00:53, 36.14it/s, now=None]\u001b[A\n",
            "t:  39%|███▉      | 1228/3147 [00:29<01:00, 31.70it/s, now=None]\u001b[A\n",
            "t:  39%|███▉      | 1235/3147 [00:29<00:50, 37.87it/s, now=None]\u001b[A\n",
            "t:  39%|███▉      | 1240/3147 [00:29<00:48, 39.60it/s, now=None]\u001b[A\n",
            "t:  40%|███▉      | 1245/3147 [00:30<00:56, 33.85it/s, now=None]\u001b[A\n",
            "t:  40%|███▉      | 1250/3147 [00:30<00:56, 33.76it/s, now=None]\u001b[A\n",
            "t:  40%|███▉      | 1255/3147 [00:30<00:52, 35.71it/s, now=None]\u001b[A\n",
            "t:  40%|████      | 1259/3147 [00:30<00:59, 31.79it/s, now=None]\u001b[A\n",
            "t:  40%|████      | 1264/3147 [00:30<00:56, 33.28it/s, now=None]\u001b[A\n",
            "t:  40%|████      | 1268/3147 [00:30<00:55, 34.08it/s, now=None]\u001b[A\n",
            "t:  40%|████      | 1272/3147 [00:30<00:57, 32.73it/s, now=None]\u001b[A\n",
            "t:  41%|████      | 1276/3147 [00:31<00:58, 31.89it/s, now=None]\u001b[A\n",
            "t:  41%|████      | 1282/3147 [00:31<00:51, 36.19it/s, now=None]\u001b[A\n",
            "t:  41%|████      | 1286/3147 [00:31<00:55, 33.60it/s, now=None]\u001b[A\n",
            "t:  41%|████      | 1290/3147 [00:31<00:53, 34.75it/s, now=None]\u001b[A\n",
            "t:  41%|████      | 1296/3147 [00:31<00:52, 35.05it/s, now=None]\u001b[A\n",
            "t:  41%|████▏     | 1302/3147 [00:31<00:46, 39.43it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1307/3147 [00:31<00:47, 38.58it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1311/3147 [00:32<00:54, 33.96it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1315/3147 [00:32<00:55, 32.99it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1321/3147 [00:32<00:48, 37.88it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1326/3147 [00:32<00:49, 36.53it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1331/3147 [00:32<00:48, 37.07it/s, now=None]\u001b[A\n",
            "t:  42%|████▏     | 1336/3147 [00:32<00:45, 39.81it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1341/3147 [00:32<00:43, 41.98it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1346/3147 [00:32<00:41, 43.60it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1351/3147 [00:32<00:40, 44.08it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1356/3147 [00:33<00:45, 39.03it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1361/3147 [00:33<00:47, 37.43it/s, now=None]\u001b[A\n",
            "t:  43%|████▎     | 1365/3147 [00:33<00:47, 37.13it/s, now=None]\u001b[A\n",
            "t:  44%|████▎     | 1369/3147 [00:33<00:51, 34.85it/s, now=None]\u001b[A\n",
            "t:  44%|████▎     | 1373/3147 [00:33<00:50, 35.03it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1377/3147 [00:33<00:52, 34.03it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1381/3147 [00:33<01:01, 28.76it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1387/3147 [00:34<00:51, 33.97it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1391/3147 [00:34<00:52, 33.38it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1395/3147 [00:34<00:50, 34.76it/s, now=None]\u001b[A\n",
            "t:  44%|████▍     | 1400/3147 [00:34<00:45, 38.05it/s, now=None]\u001b[A\n",
            "t:  45%|████▍     | 1404/3147 [00:34<00:46, 37.18it/s, now=None]\u001b[A\n",
            "t:  45%|████▍     | 1410/3147 [00:34<00:42, 40.70it/s, now=None]\u001b[A\n",
            "t:  45%|████▍     | 1415/3147 [00:34<00:42, 40.34it/s, now=None]\u001b[A\n",
            "t:  45%|████▌     | 1420/3147 [00:34<00:45, 37.67it/s, now=None]\u001b[A\n",
            "t:  45%|████▌     | 1426/3147 [00:35<00:43, 39.91it/s, now=None]\u001b[A\n",
            "t:  46%|████▌     | 1432/3147 [00:35<00:39, 43.47it/s, now=None]\u001b[A\n",
            "t:  46%|████▌     | 1437/3147 [00:35<00:44, 38.79it/s, now=None]\u001b[A\n",
            "t:  46%|████▌     | 1442/3147 [00:35<00:44, 37.94it/s, now=None]\u001b[A\n",
            "t:  46%|████▌     | 1446/3147 [00:35<00:46, 36.38it/s, now=None]\u001b[A\n",
            "t:  46%|████▌     | 1452/3147 [00:35<00:42, 40.26it/s, now=None]\u001b[A\n",
            "t:  46%|████▋     | 1457/3147 [00:35<00:42, 39.32it/s, now=None]\u001b[A\n",
            "t:  46%|████▋     | 1462/3147 [00:36<00:44, 37.87it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1466/3147 [00:36<00:47, 35.67it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1470/3147 [00:36<00:47, 34.99it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1474/3147 [00:36<00:49, 34.11it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1478/3147 [00:36<00:48, 34.47it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1483/3147 [00:36<00:45, 36.88it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1487/3147 [00:36<00:50, 32.67it/s, now=None]\u001b[A\n",
            "t:  47%|████▋     | 1492/3147 [00:36<00:52, 31.31it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1498/3147 [00:37<00:43, 37.50it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1502/3147 [00:37<00:54, 30.08it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1507/3147 [00:37<00:47, 34.22it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1511/3147 [00:37<00:53, 30.42it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1517/3147 [00:37<00:45, 36.14it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1521/3147 [00:37<00:48, 33.56it/s, now=None]\u001b[A\n",
            "t:  48%|████▊     | 1525/3147 [00:37<00:47, 34.22it/s, now=None]\u001b[A\n",
            "t:  49%|████▊     | 1529/3147 [00:38<00:46, 34.72it/s, now=None]\u001b[A\n",
            "t:  49%|████▉     | 1536/3147 [00:38<00:40, 39.99it/s, now=None]\u001b[A\n",
            "t:  49%|████▉     | 1541/3147 [00:38<00:40, 39.30it/s, now=None]\u001b[A\n",
            "t:  49%|████▉     | 1545/3147 [00:38<00:42, 37.87it/s, now=None]\u001b[A\n",
            "t:  49%|████▉     | 1551/3147 [00:38<00:37, 42.74it/s, now=None]\u001b[A\n",
            "t:  49%|████▉     | 1556/3147 [00:38<00:38, 41.11it/s, now=None]\u001b[A\n",
            "t:  50%|████▉     | 1561/3147 [00:38<00:46, 34.26it/s, now=None]\u001b[A\n",
            "t:  50%|████▉     | 1565/3147 [00:38<00:48, 32.96it/s, now=None]\u001b[A\n",
            "t:  50%|████▉     | 1570/3147 [00:39<00:44, 35.64it/s, now=None]\u001b[A\n",
            "t:  50%|█████     | 1574/3147 [00:39<00:49, 31.51it/s, now=None]\u001b[A\n",
            "t:  50%|█████     | 1581/3147 [00:39<00:45, 34.29it/s, now=None]\u001b[A\n",
            "t:  50%|█████     | 1585/3147 [00:39<00:44, 35.28it/s, now=None]\u001b[A\n",
            "t:  50%|█████     | 1589/3147 [00:39<00:46, 33.52it/s, now=None]\u001b[A\n",
            "t:  51%|█████     | 1595/3147 [00:39<00:39, 39.20it/s, now=None]\u001b[A\n",
            "t:  51%|█████     | 1600/3147 [00:39<00:43, 35.37it/s, now=None]\u001b[A\n",
            "t:  51%|█████     | 1604/3147 [00:40<00:45, 33.71it/s, now=None]\u001b[A\n",
            "t:  51%|█████     | 1608/3147 [00:40<00:48, 31.75it/s, now=None]\u001b[A\n",
            "t:  51%|█████     | 1612/3147 [00:40<00:50, 30.16it/s, now=None]\u001b[A\n",
            "t:  51%|█████▏    | 1617/3147 [00:40<00:47, 32.25it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1622/3147 [00:40<00:42, 36.15it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1626/3147 [00:40<00:41, 36.47it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1632/3147 [00:40<00:43, 34.62it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1637/3147 [00:41<00:40, 37.68it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1641/3147 [00:41<00:39, 38.14it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1646/3147 [00:41<00:40, 37.50it/s, now=None]\u001b[A\n",
            "t:  52%|█████▏    | 1652/3147 [00:41<00:39, 38.02it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1656/3147 [00:41<00:40, 36.45it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1660/3147 [00:41<00:41, 35.87it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1666/3147 [00:41<00:40, 36.78it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1671/3147 [00:41<00:38, 38.64it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1675/3147 [00:42<00:42, 34.58it/s, now=None]\u001b[A\n",
            "t:  53%|█████▎    | 1680/3147 [00:42<00:39, 37.04it/s, now=None]\u001b[A\n",
            "t:  54%|█████▎    | 1686/3147 [00:42<00:34, 41.95it/s, now=None]\u001b[A\n",
            "t:  54%|█████▍    | 1692/3147 [00:42<00:32, 44.86it/s, now=None]\u001b[A\n",
            "t:  54%|█████▍    | 1697/3147 [00:42<00:32, 44.53it/s, now=None]\u001b[A\n",
            "t:  54%|█████▍    | 1702/3147 [00:42<00:32, 44.29it/s, now=None]\u001b[A\n",
            "t:  54%|█████▍    | 1707/3147 [00:42<00:36, 39.05it/s, now=None]\u001b[A\n",
            "t:  54%|█████▍    | 1712/3147 [00:42<00:38, 37.68it/s, now=None]\u001b[A\n",
            "t:  55%|█████▍    | 1717/3147 [00:43<00:35, 39.87it/s, now=None]\u001b[A\n",
            "t:  55%|█████▍    | 1722/3147 [00:43<00:35, 40.55it/s, now=None]\u001b[A\n",
            "t:  55%|█████▍    | 1727/3147 [00:43<00:33, 42.54it/s, now=None]\u001b[A\n",
            "t:  55%|█████▌    | 1732/3147 [00:43<00:37, 37.81it/s, now=None]\u001b[A\n",
            "t:  55%|█████▌    | 1736/3147 [00:43<00:39, 36.11it/s, now=None]\u001b[A\n",
            "t:  55%|█████▌    | 1741/3147 [00:43<00:35, 39.42it/s, now=None]\u001b[A\n",
            "t:  55%|█████▌    | 1746/3147 [00:43<00:46, 30.16it/s, now=None]\u001b[A\n",
            "t:  56%|█████▌    | 1753/3147 [00:44<00:40, 34.79it/s, now=None]\u001b[A\n",
            "t:  56%|█████▌    | 1759/3147 [00:44<00:36, 38.01it/s, now=None]\u001b[A\n",
            "t:  56%|█████▌    | 1764/3147 [00:44<00:36, 37.58it/s, now=None]\u001b[A\n",
            "t:  56%|█████▌    | 1769/3147 [00:44<00:35, 39.36it/s, now=None]\u001b[A\n",
            "t:  56%|█████▋    | 1774/3147 [00:44<00:40, 34.28it/s, now=None]\u001b[A\n",
            "t:  56%|█████▋    | 1778/3147 [00:44<00:43, 31.54it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1784/3147 [00:45<00:43, 31.59it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1788/3147 [00:45<00:41, 32.92it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1793/3147 [00:45<00:40, 33.17it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1798/3147 [00:45<00:37, 35.87it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1802/3147 [00:45<00:39, 34.03it/s, now=None]\u001b[A\n",
            "t:  57%|█████▋    | 1807/3147 [00:45<00:38, 34.98it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1812/3147 [00:45<00:35, 38.12it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1816/3147 [00:45<00:40, 32.71it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1822/3147 [00:46<00:38, 34.17it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1827/3147 [00:46<00:36, 36.41it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1831/3147 [00:46<00:40, 32.23it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1835/3147 [00:46<00:41, 31.46it/s, now=None]\u001b[A\n",
            "t:  58%|█████▊    | 1839/3147 [00:46<00:42, 30.86it/s, now=None]\u001b[A\n",
            "t:  59%|█████▊    | 1846/3147 [00:46<00:39, 33.16it/s, now=None]\u001b[A\n",
            "t:  59%|█████▉    | 1850/3147 [00:46<00:37, 34.26it/s, now=None]\u001b[A\n",
            "t:  59%|█████▉    | 1854/3147 [00:47<00:41, 31.24it/s, now=None]\u001b[A\n",
            "t:  59%|█████▉    | 1859/3147 [00:47<00:39, 32.34it/s, now=None]\u001b[A\n",
            "t:  59%|█████▉    | 1865/3147 [00:47<00:36, 34.66it/s, now=None]\u001b[A\n",
            "t:  59%|█████▉    | 1869/3147 [00:47<00:38, 33.27it/s, now=None]\u001b[A\n",
            "t:  60%|█████▉    | 1874/3147 [00:47<00:40, 31.54it/s, now=None]\u001b[A\n",
            "t:  60%|█████▉    | 1880/3147 [00:47<00:35, 36.09it/s, now=None]\u001b[A\n",
            "t:  60%|█████▉    | 1884/3147 [00:47<00:35, 35.36it/s, now=None]\u001b[A\n",
            "t:  60%|█████▉    | 1888/3147 [00:48<00:37, 33.57it/s, now=None]\u001b[A\n",
            "t:  60%|██████    | 1893/3147 [00:48<00:34, 36.55it/s, now=None]\u001b[A\n",
            "t:  60%|██████    | 1897/3147 [00:48<00:40, 30.88it/s, now=None]\u001b[A\n",
            "t:  60%|██████    | 1902/3147 [00:48<00:35, 34.68it/s, now=None]\u001b[A\n",
            "t:  61%|██████    | 1906/3147 [00:48<00:41, 30.19it/s, now=None]\u001b[A\n",
            "t:  61%|██████    | 1910/3147 [00:48<00:42, 29.18it/s, now=None]\u001b[A\n",
            "t:  61%|██████    | 1916/3147 [00:48<00:35, 34.70it/s, now=None]\u001b[A\n",
            "t:  61%|██████    | 1920/3147 [00:49<00:41, 29.52it/s, now=None]\u001b[A\n",
            "t:  61%|██████    | 1924/3147 [00:49<00:40, 30.14it/s, now=None]\u001b[A\n",
            "t:  61%|██████▏   | 1928/3147 [00:49<00:38, 31.90it/s, now=None]\u001b[A\n",
            "t:  61%|██████▏   | 1932/3147 [00:49<00:39, 31.04it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1936/3147 [00:49<00:39, 30.56it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1940/3147 [00:49<00:37, 31.79it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1944/3147 [00:49<00:39, 30.82it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1948/3147 [00:49<00:37, 31.59it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1953/3147 [00:50<00:33, 35.56it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1958/3147 [00:50<00:37, 31.94it/s, now=None]\u001b[A\n",
            "t:  62%|██████▏   | 1964/3147 [00:50<00:37, 31.23it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1969/3147 [00:50<00:34, 34.28it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1973/3147 [00:50<00:36, 32.09it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1977/3147 [00:50<00:36, 32.24it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1981/3147 [00:51<00:40, 28.96it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1987/3147 [00:51<00:33, 34.97it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1991/3147 [00:51<00:37, 31.01it/s, now=None]\u001b[A\n",
            "t:  63%|██████▎   | 1995/3147 [00:51<00:38, 30.23it/s, now=None]\u001b[A\n",
            "t:  64%|██████▎   | 1999/3147 [00:51<00:35, 32.36it/s, now=None]\u001b[A\n",
            "t:  64%|██████▎   | 2004/3147 [00:51<00:33, 33.78it/s, now=None]\u001b[A\n",
            "t:  64%|██████▍   | 2009/3147 [00:51<00:33, 34.09it/s, now=None]\u001b[A\n",
            "t:  64%|██████▍   | 2015/3147 [00:51<00:28, 39.48it/s, now=None]\u001b[A\n",
            "t:  64%|██████▍   | 2020/3147 [00:52<00:31, 35.49it/s, now=None]\u001b[A\n",
            "t:  64%|██████▍   | 2024/3147 [00:52<00:31, 35.93it/s, now=None]\u001b[A\n",
            "t:  64%|██████▍   | 2029/3147 [00:52<00:30, 36.54it/s, now=None]\u001b[A\n",
            "t:  65%|██████▍   | 2035/3147 [00:52<00:27, 40.65it/s, now=None]\u001b[A\n",
            "t:  65%|██████▍   | 2040/3147 [00:52<00:26, 42.11it/s, now=None]\u001b[A\n",
            "t:  65%|██████▍   | 2045/3147 [00:52<00:30, 36.39it/s, now=None]\u001b[A\n",
            "t:  65%|██████▌   | 2049/3147 [00:52<00:30, 36.54it/s, now=None]\u001b[A\n",
            "t:  65%|██████▌   | 2054/3147 [00:53<00:30, 35.37it/s, now=None]\u001b[A\n",
            "t:  65%|██████▌   | 2060/3147 [00:53<00:28, 38.46it/s, now=None]\u001b[A\n",
            "t:  66%|██████▌   | 2066/3147 [00:53<00:29, 36.46it/s, now=None]\u001b[A\n",
            "t:  66%|██████▌   | 2072/3147 [00:53<00:29, 36.98it/s, now=None]\u001b[A\n",
            "t:  66%|██████▌   | 2078/3147 [00:53<00:25, 41.55it/s, now=None]\u001b[A\n",
            "t:  66%|██████▌   | 2083/3147 [00:53<00:25, 41.49it/s, now=None]\u001b[A\n",
            "t:  66%|██████▋   | 2088/3147 [00:53<00:24, 42.61it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2093/3147 [00:54<00:28, 37.24it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2098/3147 [00:54<00:26, 39.96it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2104/3147 [00:54<00:25, 41.45it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2109/3147 [00:54<00:30, 33.79it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2113/3147 [00:54<00:29, 34.96it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2117/3147 [00:54<00:30, 33.96it/s, now=None]\u001b[A\n",
            "t:  67%|██████▋   | 2121/3147 [00:54<00:29, 35.19it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2125/3147 [00:54<00:29, 34.53it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2129/3147 [00:55<00:29, 34.27it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2133/3147 [00:55<00:29, 34.02it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2140/3147 [00:55<00:27, 36.31it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2145/3147 [00:55<00:25, 39.52it/s, now=None]\u001b[A\n",
            "t:  68%|██████▊   | 2150/3147 [00:55<00:32, 30.59it/s, now=None]\u001b[A\n",
            "t:  69%|██████▊   | 2157/3147 [00:55<00:30, 32.72it/s, now=None]\u001b[A\n",
            "t:  69%|██████▊   | 2161/3147 [00:55<00:29, 33.34it/s, now=None]\u001b[A\n",
            "t:  69%|██████▉   | 2165/3147 [00:56<00:32, 30.38it/s, now=None]\u001b[A\n",
            "t:  69%|██████▉   | 2170/3147 [00:56<00:32, 29.98it/s, now=None]\u001b[A\n",
            "t:  69%|██████▉   | 2176/3147 [00:56<00:27, 35.26it/s, now=None]\u001b[A\n",
            "t:  69%|██████▉   | 2180/3147 [00:56<00:29, 33.00it/s, now=None]\u001b[A\n",
            "t:  69%|██████▉   | 2185/3147 [00:56<00:28, 33.81it/s, now=None]\u001b[A\n",
            "t:  70%|██████▉   | 2189/3147 [00:56<00:29, 32.87it/s, now=None]\u001b[A\n",
            "t:  70%|██████▉   | 2193/3147 [00:56<00:29, 32.47it/s, now=None]\u001b[A\n",
            "t:  70%|██████▉   | 2199/3147 [00:57<00:27, 34.57it/s, now=None]\u001b[A\n",
            "t:  70%|███████   | 2204/3147 [00:57<00:24, 37.84it/s, now=None]\u001b[A\n",
            "t:  70%|███████   | 2208/3147 [00:57<00:26, 35.65it/s, now=None]\u001b[A\n",
            "t:  70%|███████   | 2214/3147 [00:57<00:24, 37.84it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2220/3147 [00:57<00:22, 41.78it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2225/3147 [00:57<00:22, 41.63it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2230/3147 [00:57<00:23, 39.43it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2234/3147 [00:58<00:24, 38.03it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2238/3147 [00:58<00:25, 35.33it/s, now=None]\u001b[A\n",
            "t:  71%|███████   | 2242/3147 [00:58<00:24, 36.33it/s, now=None]\u001b[A\n",
            "t:  71%|███████▏  | 2247/3147 [00:58<00:22, 39.78it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2252/3147 [00:58<00:26, 33.15it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2257/3147 [00:58<00:25, 34.91it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2261/3147 [00:58<00:25, 34.93it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2265/3147 [00:58<00:25, 34.44it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2269/3147 [00:59<00:25, 34.98it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2273/3147 [00:59<00:26, 32.42it/s, now=None]\u001b[A\n",
            "t:  72%|███████▏  | 2277/3147 [00:59<00:34, 25.57it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2283/3147 [00:59<00:31, 27.26it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2289/3147 [00:59<00:30, 28.30it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2295/3147 [00:59<00:26, 32.17it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2299/3147 [01:00<00:26, 32.13it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2303/3147 [01:00<00:25, 32.83it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2307/3147 [01:00<00:26, 31.80it/s, now=None]\u001b[A\n",
            "t:  73%|███████▎  | 2311/3147 [01:00<00:25, 32.79it/s, now=None]\u001b[A\n",
            "t:  74%|███████▎  | 2315/3147 [01:00<00:26, 31.90it/s, now=None]\u001b[A\n",
            "t:  74%|███████▎  | 2319/3147 [01:00<00:27, 30.52it/s, now=None]\u001b[A\n",
            "t:  74%|███████▍  | 2324/3147 [01:00<00:25, 31.81it/s, now=None]\u001b[A\n",
            "t:  74%|███████▍  | 2329/3147 [01:00<00:23, 34.68it/s, now=None]\u001b[A\n",
            "t:  74%|███████▍  | 2333/3147 [01:01<00:25, 32.33it/s, now=None]\u001b[A\n",
            "t:  74%|███████▍  | 2338/3147 [01:01<00:25, 31.35it/s, now=None]\u001b[A\n",
            "t:  74%|███████▍  | 2343/3147 [01:01<00:22, 35.49it/s, now=None]\u001b[A\n",
            "t:  75%|███████▍  | 2347/3147 [01:01<00:23, 33.66it/s, now=None]\u001b[A\n",
            "t:  75%|███████▍  | 2351/3147 [01:01<00:27, 29.35it/s, now=None]\u001b[A\n",
            "t:  75%|███████▍  | 2357/3147 [01:01<00:25, 31.04it/s, now=None]\u001b[A\n",
            "t:  75%|███████▌  | 2361/3147 [01:01<00:24, 32.62it/s, now=None]\u001b[A\n",
            "t:  75%|███████▌  | 2365/3147 [01:02<00:24, 32.35it/s, now=None]\u001b[A\n",
            "t:  75%|███████▌  | 2369/3147 [01:02<00:24, 32.02it/s, now=None]\u001b[A\n",
            "t:  75%|███████▌  | 2375/3147 [01:02<00:21, 35.19it/s, now=None]\u001b[A\n",
            "t:  76%|███████▌  | 2381/3147 [01:02<00:20, 36.53it/s, now=None]\u001b[A\n",
            "t:  76%|███████▌  | 2386/3147 [01:02<00:20, 37.95it/s, now=None]\u001b[A\n",
            "t:  76%|███████▌  | 2391/3147 [01:02<00:19, 37.83it/s, now=None]\u001b[A\n",
            "t:  76%|███████▌  | 2395/3147 [01:02<00:21, 35.69it/s, now=None]\u001b[A\n",
            "t:  76%|███████▋  | 2400/3147 [01:03<00:19, 38.55it/s, now=None]\u001b[A\n",
            "t:  76%|███████▋  | 2404/3147 [01:03<00:20, 36.08it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2408/3147 [01:03<00:20, 36.17it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2412/3147 [01:03<00:20, 36.01it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2418/3147 [01:03<00:17, 42.17it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2423/3147 [01:03<00:21, 34.10it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2427/3147 [01:03<00:21, 34.21it/s, now=None]\u001b[A\n",
            "t:  77%|███████▋  | 2434/3147 [01:03<00:20, 35.26it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2439/3147 [01:04<00:18, 37.27it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2444/3147 [01:04<00:17, 39.33it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2450/3147 [01:04<00:17, 39.59it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2456/3147 [01:04<00:17, 40.36it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2461/3147 [01:04<00:18, 37.47it/s, now=None]\u001b[A\n",
            "t:  78%|███████▊  | 2465/3147 [01:04<00:17, 38.04it/s, now=None]\u001b[A\n",
            "t:  79%|███████▊  | 2471/3147 [01:04<00:15, 42.80it/s, now=None]\u001b[A\n",
            "t:  79%|███████▊  | 2476/3147 [01:04<00:15, 42.19it/s, now=None]\u001b[A\n",
            "t:  79%|███████▉  | 2482/3147 [01:05<00:14, 46.69it/s, now=None]\u001b[A\n",
            "t:  79%|███████▉  | 2487/3147 [01:05<00:15, 41.78it/s, now=None]\u001b[A\n",
            "t:  79%|███████▉  | 2493/3147 [01:05<00:14, 43.65it/s, now=None]\u001b[A\n",
            "t:  79%|███████▉  | 2498/3147 [01:05<00:16, 38.84it/s, now=None]\u001b[A\n",
            "t:  80%|███████▉  | 2503/3147 [01:05<00:16, 39.53it/s, now=None]\u001b[A\n",
            "t:  80%|███████▉  | 2509/3147 [01:05<00:14, 44.03it/s, now=None]\u001b[A\n",
            "t:  80%|███████▉  | 2514/3147 [01:05<00:14, 42.76it/s, now=None]\u001b[A\n",
            "t:  80%|████████  | 2520/3147 [01:06<00:18, 34.29it/s, now=None]\u001b[A\n",
            "t:  80%|████████  | 2525/3147 [01:06<00:17, 36.41it/s, now=None]\u001b[A\n",
            "t:  80%|████████  | 2529/3147 [01:06<00:18, 33.56it/s, now=None]\u001b[A\n",
            "t:  80%|████████  | 2533/3147 [01:06<00:18, 33.97it/s, now=None]\u001b[A\n",
            "t:  81%|████████  | 2538/3147 [01:06<00:18, 32.06it/s, now=None]\u001b[A\n",
            "t:  81%|████████  | 2544/3147 [01:06<00:18, 33.15it/s, now=None]\u001b[A\n",
            "t:  81%|████████  | 2549/3147 [01:07<00:18, 33.09it/s, now=None]\u001b[A\n",
            "t:  81%|████████  | 2554/3147 [01:07<00:16, 35.73it/s, now=None]\u001b[A\n",
            "t:  81%|████████▏ | 2558/3147 [01:07<00:17, 34.58it/s, now=None]\u001b[A\n",
            "t:  81%|████████▏ | 2564/3147 [01:07<00:14, 39.73it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2569/3147 [01:07<00:15, 36.21it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2573/3147 [01:07<00:16, 34.60it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2577/3147 [01:07<00:16, 35.57it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2582/3147 [01:07<00:14, 38.69it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2587/3147 [01:07<00:13, 40.90it/s, now=None]\u001b[A\n",
            "t:  82%|████████▏ | 2593/3147 [01:08<00:12, 44.51it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2598/3147 [01:08<00:12, 44.22it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2603/3147 [01:08<00:12, 42.50it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2608/3147 [01:08<00:12, 42.16it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2614/3147 [01:08<00:11, 45.34it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2619/3147 [01:08<00:12, 42.34it/s, now=None]\u001b[A\n",
            "t:  83%|████████▎ | 2624/3147 [01:08<00:13, 39.66it/s, now=None]\u001b[A\n",
            "t:  84%|████████▎ | 2630/3147 [01:08<00:11, 44.12it/s, now=None]\u001b[A\n",
            "t:  84%|████████▎ | 2635/3147 [01:09<00:11, 44.23it/s, now=None]\u001b[A\n",
            "t:  84%|████████▍ | 2640/3147 [01:09<00:11, 45.34it/s, now=None]\u001b[A\n",
            "t:  84%|████████▍ | 2646/3147 [01:09<00:12, 39.63it/s, now=None]\u001b[A\n",
            "t:  84%|████████▍ | 2653/3147 [01:09<00:10, 45.59it/s, now=None]\u001b[A\n",
            "t:  84%|████████▍ | 2658/3147 [01:09<00:11, 43.02it/s, now=None]\u001b[A\n",
            "t:  85%|████████▍ | 2664/3147 [01:09<00:10, 46.79it/s, now=None]\u001b[A\n",
            "t:  85%|████████▍ | 2670/3147 [01:09<00:09, 48.95it/s, now=None]\u001b[A\n",
            "t:  85%|████████▌ | 2676/3147 [01:09<00:10, 47.01it/s, now=None]\u001b[A\n",
            "t:  85%|████████▌ | 2681/3147 [01:10<00:11, 42.15it/s, now=None]\u001b[A\n",
            "t:  85%|████████▌ | 2686/3147 [01:10<00:10, 43.40it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 2691/3147 [01:10<00:10, 44.62it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 2696/3147 [01:10<00:10, 43.67it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 2701/3147 [01:10<00:09, 45.13it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 2706/3147 [01:10<00:09, 45.07it/s, now=None]\u001b[A\n",
            "t:  86%|████████▌ | 2711/3147 [01:10<00:09, 44.70it/s, now=None]\u001b[A\n",
            "t:  86%|████████▋ | 2717/3147 [01:10<00:08, 48.87it/s, now=None]\u001b[A\n",
            "t:  86%|████████▋ | 2722/3147 [01:10<00:08, 48.49it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2727/3147 [01:11<00:11, 36.00it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2732/3147 [01:11<00:12, 34.11it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2736/3147 [01:11<00:11, 35.02it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2740/3147 [01:11<00:11, 34.93it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2745/3147 [01:11<00:10, 38.56it/s, now=None]\u001b[A\n",
            "t:  87%|████████▋ | 2750/3147 [01:11<00:09, 41.20it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2755/3147 [01:11<00:09, 42.99it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2760/3147 [01:12<00:09, 41.21it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2766/3147 [01:12<00:08, 44.26it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2771/3147 [01:12<00:09, 38.77it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2777/3147 [01:12<00:08, 42.13it/s, now=None]\u001b[A\n",
            "t:  88%|████████▊ | 2782/3147 [01:12<00:10, 36.33it/s, now=None]\u001b[A\n",
            "t:  89%|████████▊ | 2789/3147 [01:12<00:08, 42.89it/s, now=None]\u001b[A\n",
            "t:  89%|████████▉ | 2794/3147 [01:12<00:08, 41.14it/s, now=None]\u001b[A\n",
            "t:  89%|████████▉ | 2801/3147 [01:13<00:07, 43.26it/s, now=None]\u001b[A\n",
            "t:  89%|████████▉ | 2806/3147 [01:13<00:07, 42.77it/s, now=None]\u001b[A\n",
            "t:  89%|████████▉ | 2812/3147 [01:13<00:07, 43.96it/s, now=None]\u001b[A\n",
            "t:  90%|████████▉ | 2818/3147 [01:13<00:07, 41.28it/s, now=None]\u001b[A\n",
            "t:  90%|████████▉ | 2823/3147 [01:13<00:07, 41.97it/s, now=None]\u001b[A\n",
            "t:  90%|████████▉ | 2828/3147 [01:13<00:09, 35.15it/s, now=None]\u001b[A\n",
            "t:  90%|████████▉ | 2832/3147 [01:13<00:08, 36.15it/s, now=None]\u001b[A\n",
            "t:  90%|█████████ | 2836/3147 [01:13<00:08, 36.19it/s, now=None]\u001b[A\n",
            "t:  90%|█████████ | 2841/3147 [01:14<00:07, 39.29it/s, now=None]\u001b[A\n",
            "t:  90%|█████████ | 2846/3147 [01:14<00:07, 38.85it/s, now=None]\u001b[A\n",
            "t:  91%|█████████ | 2850/3147 [01:14<00:08, 33.39it/s, now=None]\u001b[A\n",
            "t:  91%|█████████ | 2856/3147 [01:14<00:07, 39.46it/s, now=None]\u001b[A\n",
            "t:  91%|█████████ | 2861/3147 [01:14<00:07, 38.40it/s, now=None]\u001b[A\n",
            "t:  91%|█████████ | 2866/3147 [01:14<00:07, 37.42it/s, now=None]\u001b[A\n",
            "t:  91%|█████████▏| 2872/3147 [01:14<00:07, 38.92it/s, now=None]\u001b[A\n",
            "t:  91%|█████████▏| 2879/3147 [01:15<00:06, 43.29it/s, now=None]\u001b[A\n",
            "t:  92%|█████████▏| 2884/3147 [01:15<00:06, 43.19it/s, now=None]\u001b[A\n",
            "t:  92%|█████████▏| 2889/3147 [01:15<00:06, 38.85it/s, now=None]\u001b[A\n",
            "t:  92%|█████████▏| 2895/3147 [01:15<00:06, 39.40it/s, now=None]\u001b[A\n",
            "t:  92%|█████████▏| 2901/3147 [01:15<00:05, 43.48it/s, now=None]\u001b[A\n",
            "t:  92%|█████████▏| 2906/3147 [01:15<00:06, 40.15it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2911/3147 [01:15<00:05, 40.51it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2916/3147 [01:15<00:05, 39.40it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2922/3147 [01:16<00:05, 41.20it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2927/3147 [01:16<00:05, 37.44it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2931/3147 [01:16<00:05, 36.21it/s, now=None]\u001b[A\n",
            "t:  93%|█████████▎| 2938/3147 [01:16<00:04, 43.88it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▎| 2943/3147 [01:16<00:04, 41.98it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▎| 2948/3147 [01:16<00:04, 41.87it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▍| 2953/3147 [01:16<00:05, 37.04it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▍| 2959/3147 [01:17<00:05, 35.75it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▍| 2964/3147 [01:17<00:05, 36.17it/s, now=None]\u001b[A\n",
            "t:  94%|█████████▍| 2970/3147 [01:17<00:04, 37.14it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▍| 2977/3147 [01:17<00:04, 42.46it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▍| 2982/3147 [01:17<00:04, 39.55it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▍| 2988/3147 [01:17<00:03, 43.77it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▌| 2993/3147 [01:17<00:03, 40.59it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▌| 2999/3147 [01:17<00:03, 44.62it/s, now=None]\u001b[A\n",
            "t:  95%|█████████▌| 3004/3147 [01:18<00:03, 43.07it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▌| 3010/3147 [01:18<00:03, 44.72it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▌| 3015/3147 [01:18<00:03, 39.18it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▌| 3021/3147 [01:18<00:03, 40.89it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▌| 3026/3147 [01:18<00:02, 42.85it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▋| 3031/3147 [01:18<00:02, 40.39it/s, now=None]\u001b[A\n",
            "t:  96%|█████████▋| 3036/3147 [01:18<00:02, 40.86it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3041/3147 [01:19<00:02, 35.96it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3045/3147 [01:19<00:03, 33.50it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3049/3147 [01:19<00:03, 31.02it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3053/3147 [01:19<00:03, 27.85it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3058/3147 [01:19<00:02, 30.93it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3062/3147 [01:19<00:02, 31.11it/s, now=None]\u001b[A\n",
            "t:  97%|█████████▋| 3067/3147 [01:19<00:02, 34.81it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3071/3147 [01:20<00:02, 36.01it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3075/3147 [01:20<00:02, 33.93it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3081/3147 [01:20<00:02, 32.79it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3087/3147 [01:20<00:01, 37.86it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3091/3147 [01:20<00:01, 31.02it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3095/3147 [01:20<00:01, 30.13it/s, now=None]\u001b[A\n",
            "t:  98%|█████████▊| 3099/3147 [01:20<00:01, 30.78it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▊| 3103/3147 [01:21<00:01, 32.29it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▊| 3107/3147 [01:21<00:01, 32.43it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▉| 3113/3147 [01:21<00:00, 37.18it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▉| 3117/3147 [01:21<00:00, 32.85it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▉| 3122/3147 [01:21<00:00, 36.12it/s, now=None]\u001b[A\n",
            "t:  99%|█████████▉| 3128/3147 [01:21<00:00, 34.05it/s, now=None]\u001b[A\n",
            "t: 100%|█████████▉| 3132/3147 [01:21<00:00, 35.06it/s, now=None]\u001b[A\n",
            "t: 100%|█████████▉| 3136/3147 [01:21<00:00, 35.12it/s, now=None]\u001b[A\n",
            "t: 100%|█████████▉| 3141/3147 [01:22<00:00, 37.29it/s, now=None]\u001b[A\n",
            "t: 100%|█████████▉| 3145/3147 [01:22<00:00, 36.21it/s, now=None]\u001b[A\n",
            "chunk:  73%|███████▎  | 1020/1390 [21:56<00:01, 285.04it/s, now=None]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready Summary/JungleVideo.mp4\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import os\n",
        "import numpy as np\n",
        "def summary(video_name, video_file_path, yArray, summ_ratio=0.1):\n",
        "  print(len(yArray))\n",
        "  temp = []\n",
        "  for i, pp in enumerate(yArray[:-1]):\n",
        "    temp.append((pp, i))\n",
        "  \n",
        "  C = np.ones(len(yArray))\n",
        "  temp.sort(reverse=True)\n",
        "  representative_points = []\n",
        "  for i in range(int(summ_ratio * len(temp))):\n",
        "    representative_points.append(temp[i][1])\n",
        "  del temp\n",
        "  representative_points.sort()\n",
        "  print(representative_points)\n",
        "  video_file_path = video_file_path + '/' +video_name\n",
        "  video_clip = VideoFileClip(video_file_path)\n",
        "  shotIdx = np.concatenate(([0], np.cumsum(C[:-1])))\n",
        "  # frames_per_seconds = np.sum(C)/ video_clip.duration\n",
        "  frames_per_seconds = 1\n",
        "  chosen_shots_clips = []\n",
        "  for i in range(len(representative_points)):\n",
        "      curr_start_time = shotIdx[representative_points[i]] / frames_per_seconds  # [Sec]\n",
        "      if representative_points[i] == (shotIdx.__len__() - 1):\n",
        "          curr_end_time = video_clip.duration\n",
        "      else:\n",
        "          curr_end_time = (shotIdx[representative_points[i] + 1] - 1) / frames_per_seconds  # [Sec]\n",
        "      chosen_shots_clips.append(VideoFileClip(video_file_path).subclip(curr_start_time, curr_end_time + 1))\n",
        "  if chosen_shots_clips == []:\n",
        "      print(\"The length of the shortest shots exceeds the allotted summarization time\")\n",
        "  else:\n",
        "      # for i in range(len(chosen_shots_clips)):\n",
        "        # chosen_shots_clips[i].write_videofile(os.path.join('Summary', str(i) + \"_result.mp4\"))\n",
        "\n",
        "      summ_clip = concatenate_videoclips(chosen_shots_clips)\n",
        " \n",
        "\n",
        "  summ_clip.write_videofile(os.path.join('Summary', video_name[:len(video_name) - 4] + \".mp4\"))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Video Name too long!\n",
        "  # Reformat Video name -> zip file name + _number\n",
        "  \n",
        "  npy_vid_name = \"JungleVideo\" + \".npy\"\n",
        "  yArr = np.load('/content/drive/MyDrive/MSVA/yResults/' + npy_vid_name)\n",
        "  vid_name = npy_vid_name[:len(npy_vid_name) - 4] + \".mp4\"\n",
        "  summary(vid_name, '/content/drive/MyDrive/MSVA/samplevideo', yArr)\n",
        "  print('Done')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAY91WIfzBJF"
      },
      "source": [
        "JSON 파일 한글로 인코딩하여 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te6gSsz-UbFh",
        "outputId": "5995d14f-8a39-4858-cb1b-d48cc32bc257"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'filename': '16bb6c3f63f4e87366b553ce881a8ba286882e052e17c1a70661a5daec5d06e9-유튜브 영상물(2~5분) 장면별 중요도 태그-100176-20210108173536-001-001', 'category': '영화/애니메이션', 'length': 267.417, 'quality': '720p', 'license': 'Youtube', 'ad_start': '-', 'ad_end': '-', 'three_secs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89], 'path': '093.비디오 요약 영상 데이터/01.데이터/7.비디오 요약 영상 데이터(100% 이상, 구문적 정확성 검증용)/2~5분', 'annots': [{'annotator_id': '6359559137395038208', 'highlight': [11, 35, 54, 77], 'represent': [35, 77]}, {'annotator_id': '7078990615133829120', 'highlight': [14, 34, 63, 76], 'represent': [34, 76]}, {'annotator_id': '8135871289035561984', 'highlight': [5, 22, 44, 66], 'represent': [5, 66]}, {'annotator_id': '8623417869944868864', 'highlight': [11, 34, 51, 79], 'represent': [34, 79]}, {'annotator_id': '3756140495494919168', 'highlight': [1, 14, 37, 64], 'represent': [1, 37]}, {'annotator_id': '8024652220360539136', 'highlight': [1, 9, 18, 37], 'represent': [9, 18]}, {'annotator_id': '6361876098004019200', 'highlight': [3, 18, 58, 82], 'represent': [58, 82]}, {'annotator_id': '605072627766851584', 'highlight': [12, 26, 44, 81], 'represent': [12, 44]}, {'annotator_id': '3799481466110034944', 'highlight': [14, 32, 48, 75], 'represent': [14, 75]}, {'annotator_id': '8785347035908957184', 'highlight': [2, 22, 38, 84], 'represent': [2, 84]}]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open('video_summary_training_data(2~5분).json', 'r') as fp:\n",
        "    after = json.load(fp)\n",
        "    print(after[0])\n",
        "    print(\"Hello world ;t kavblkjnagajb\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8j4sH0sLkl1"
      },
      "outputs": [],
      "source": [
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7GdbXxVT9wr",
        "outputId": "e6b73f26-a7dd-485e-e752-010e1e9cfea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19389, 1024)\n",
            "cjibtmSLxQ4.npy (19389, 1024)\n",
            "(4914, 1024)\n",
            "eQu1rNs0an0.npy (4914, 1024)\n",
            "(6224, 1024)\n",
            "LRw_obCPUt0.npy (6224, 1024)\n",
            "(17510, 1024)\n",
            "fWutDQy1nnY.npy (17510, 1024)\n",
            "(10900, 1024)\n",
            "RBCABdttQmI.npy (10900, 1024)\n",
            "(6563, 1024)\n",
            "PJrm840pAUI.npy (6563, 1024)\n",
            "(3785, 1024)\n",
            "JKpqYvAdIsw.npy (3785, 1024)\n",
            "(9853, 1024)\n",
            "vdmoEJ5YbrQ.npy (9853, 1024)\n",
            "(8056, 1024)\n",
            "qqR6AEXwxoQ.npy (8056, 1024)\n",
            "(3295, 1024)\n",
            "91IHQYk1IQM.npy (3295, 1024)\n",
            "(8264, 1024)\n",
            "z_6gVvQb2d0.npy (8264, 1024)\n",
            "(4671, 1024)\n",
            "98MoyGZKHXc.npy (4671, 1024)\n",
            "(4836, 1024)\n",
            "3eYKfiOEJNs.npy (4836, 1024)\n",
            "(4451, 1024)\n",
            "sTEELN-vY30.npy (4451, 1024)\n",
            "(3992, 1024)\n",
            "uGu_10sucQo.npy (3992, 1024)\n",
            "(6895, 1024)\n",
            "-esJrBWj2d8.npy (6895, 1024)\n",
            "(3515, 1024)\n",
            "0tmA_C6XwfM.npy (3515, 1024)\n",
            "(3688, 1024)\n",
            "byxOvuiIJV0.npy (3688, 1024)\n",
            "(4149, 1024)\n",
            "Se3oxnaPsz0.npy (4149, 1024)\n",
            "(5644, 1024)\n",
            "b626MiF1ew4.npy (5644, 1024)\n",
            "(3988, 1024)\n",
            "4wU_LUjG5Ic.npy (3988, 1024)\n",
            "(9518, 1024)\n",
            "WG0MBPpPC6I.npy (9518, 1024)\n",
            "(4287, 1024)\n",
            "JgHubY5Vw3Y.npy (4287, 1024)\n",
            "(4307, 1024)\n",
            "xxdtq8mxegs.npy (4307, 1024)\n",
            "(3879, 1024)\n",
            "kLxoNp-UchI.npy (3879, 1024)\n",
            "(4723, 1024)\n",
            "NyBmCxDoHJU.npy (4723, 1024)\n",
            "(5725, 1024)\n",
            "37rzWOQsNIw.npy (5725, 1024)\n",
            "(2924, 1024)\n",
            "EE-bNr36nyA.npy (2924, 1024)\n",
            "(5829, 1024)\n",
            "Hl-__g2gn_A.npy (5829, 1024)\n",
            "(9714, 1024)\n",
            "Yi4Ij2NM7U4.npy (9714, 1024)\n",
            "(4446, 1024)\n",
            "_xMr-HKMfVA.npy (4446, 1024)\n",
            "(9654, 1024)\n",
            "HT5vyqe0Xaw.npy (9654, 1024)\n",
            "(13494, 1024)\n",
            "Bhxk-O1Y7Ho.npy (13494, 1024)\n",
            "(10580, 1024)\n",
            "AwmHb44_ouw.npy (10580, 1024)\n",
            "(14002, 1024)\n",
            "J0nA4VgnoCo.npy (14002, 1024)\n",
            "(3978, 1024)\n",
            "akI8YFjEmUw.npy (3978, 1024)\n",
            "(3310, 1024)\n",
            "XzYM3PfTM4w.npy (3310, 1024)\n",
            "(5922, 1024)\n",
            "EYqVtI9YWJA.npy (5922, 1024)\n",
            "(4683, 1024)\n",
            "i3wAGJaaktw.npy (4683, 1024)\n",
            "(4339, 1024)\n",
            "GsAD1KT1xo8.npy (4339, 1024)\n",
            "(7942, 1024)\n",
            "WxtbjNsCQ8A.npy (7942, 1024)\n",
            "(2483, 1024)\n",
            "iVt07TCkFM0.npy (2483, 1024)\n",
            "(13348, 1024)\n",
            "xmEERLqJ2kU.npy (13348, 1024)\n",
            "(11397, 1024)\n",
            "oDXZc0tZe04.npy (11397, 1024)\n",
            "(15290, 1024)\n",
            "E11zDS9XGzg.npy (15290, 1024)\n",
            "(7193, 1024)\n",
            "gzDbaEs1Rlg.npy (7193, 1024)\n",
            "(5614, 1024)\n",
            "XkqCExn6_Us.npy (5614, 1024)\n",
            "(5395, 1024)\n",
            "VuWGsYPqAX8.npy (5395, 1024)\n",
            "(6993, 1024)\n",
            "xwqBXPGE9pQ.npy (6993, 1024)\n",
            "(5954, 1024)\n",
            "jcoYJXDG9sw.npy (5954, 1024)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "save_path = '/content/drive/MyDrive/MSVA/datasets/kinetic_features/tvsum/EMOTION/features'\n",
        "for file_name in os.listdir('./'):\n",
        "  now = np.load('./'+file_name)\n",
        "  a = now.shape\n",
        "  print(a)\n",
        "  b = np.ones((a[0], 1024))\n",
        "  np.save(os.path.join(save_path, file_name), b)\n",
        "  print(file_name, b.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOdhFcmGgIy1",
        "outputId": "de92b90c-ff1d-4451-d1e1-de6fc9699a14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0]\n",
            " [0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "random.randint(0,1)\n",
        "a = np.full((2, 3), random.randint(0,1))\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIZdZCZaT3lh",
        "outputId": "2980e925-3919-41a1-9638-f87a7bb6aa56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/MSVA/datasets/kinetic_features/tvsum/EMOTION/features\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/MSVA/datasets/kinetic_features/tvsum/EMOTION/features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtctO_NqT_JF",
        "outputId": "2c856bef-ead1-4462-c51c-eb1a24c3b0eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0tmA_C6XwfM\n",
            "(3500, 1024)\n",
            "(3515, 1024)\n",
            "-esJrBWj2d8\n",
            "(6670, 1024)\n",
            "(6895, 1024)\n",
            "37rzWOQsNIw\n",
            "(5700, 1024)\n",
            "(5725, 1024)\n",
            "4wU_LUjG5Ic\n",
            "(3984, 1024)\n",
            "(3988, 1024)\n",
            "91IHQYk1IQM\n",
            "(3190, 1024)\n",
            "(3295, 1024)\n",
            "E11zDS9XGzg\n",
            "(14790, 1024)\n",
            "(15290, 1024)\n",
            "98MoyGZKHXc\n",
            "(4650, 1024)\n",
            "(4671, 1024)\n",
            "AwmHb44_ouw\n",
            "(10560, 1024)\n",
            "(10580, 1024)\n",
            "3eYKfiOEJNs\n",
            "(4656, 1024)\n",
            "(4836, 1024)\n",
            "EE-bNr36nyA\n",
            "(2842, 1024)\n",
            "(2924, 1024)\n",
            "Bhxk-O1Y7Ho\n",
            "(13050, 1024)\n",
            "(13494, 1024)\n",
            "GsAD1KT1xo8\n",
            "(4320, 1024)\n",
            "(4339, 1024)\n",
            "NyBmCxDoHJU\n",
            "(4700, 1024)\n",
            "(4723, 1024)\n",
            "LRw_obCPUt0\n",
            "(5980, 1024)\n",
            "(6224, 1024)\n",
            "Hl-__g2gn_A\n",
            "(5808, 1024)\n",
            "(5829, 1024)\n",
            "EYqVtI9YWJA\n",
            "(5742, 1024)\n",
            "(5922, 1024)\n",
            "JgHubY5Vw3Y\n",
            "(4260, 1024)\n",
            "(4287, 1024)\n",
            "HT5vyqe0Xaw\n",
            "(9338, 1024)\n",
            "(9654, 1024)\n",
            "Se3oxnaPsz0\n",
            "(4140, 1024)\n",
            "(4149, 1024)\n",
            "J0nA4VgnoCo\n",
            "(13432, 1024)\n",
            "(14002, 1024)\n",
            "PJrm840pAUI\n",
            "(6302, 1024)\n",
            "(6563, 1024)\n",
            "JKpqYvAdIsw\n",
            "(3648, 1024)\n",
            "(3785, 1024)\n",
            "XzYM3PfTM4w\n",
            "(3300, 1024)\n",
            "(3310, 1024)\n",
            "byxOvuiIJV0\n",
            "(3542, 1024)\n",
            "(3688, 1024)\n",
            "WxtbjNsCQ8A\n",
            "(7920, 1024)\n",
            "(7942, 1024)\n",
            "XkqCExn6_Us\n",
            "(5580, 1024)\n",
            "(5614, 1024)\n",
            "b626MiF1ew4\n",
            "(5616, 1024)\n",
            "(5644, 1024)\n",
            "VuWGsYPqAX8\n",
            "(5184, 1024)\n",
            "(5395, 1024)\n",
            "WG0MBPpPC6I\n",
            "(9504, 1024)\n",
            "(9518, 1024)\n",
            "akI8YFjEmUw\n",
            "(3960, 1024)\n",
            "(3978, 1024)\n",
            "Yi4Ij2NM7U4\n",
            "(9696, 1024)\n",
            "(9714, 1024)\n",
            "_xMr-HKMfVA\n",
            "(4440, 1024)\n",
            "(4446, 1024)\n",
            "RBCABdttQmI\n",
            "(10556, 1024)\n",
            "(10900, 1024)\n",
            "oDXZc0tZe04\n",
            "(11020, 1024)\n",
            "(11397, 1024)\n",
            "uGu_10sucQo\n",
            "(3984, 1024)\n",
            "(3992, 1024)\n",
            "iVt07TCkFM0\n",
            "(2392, 1024)\n",
            "(2483, 1024)\n",
            "eQu1rNs0an0\n",
            "(4756, 1024)\n",
            "(4914, 1024)\n",
            "cjibtmSLxQ4\n",
            "(19380, 1024)\n",
            "(19389, 1024)\n",
            "qqR6AEXwxoQ\n",
            "(8040, 1024)\n",
            "(8056, 1024)\n",
            "gzDbaEs1Rlg\n",
            "(6912, 1024)\n",
            "(7193, 1024)\n",
            "i3wAGJaaktw\n",
            "(4680, 1024)\n",
            "(4683, 1024)\n",
            "sTEELN-vY30\n",
            "(4440, 1024)\n",
            "(4451, 1024)\n",
            "fWutDQy1nnY\n",
            "(16936, 1024)\n",
            "(17510, 1024)\n",
            "kLxoNp-UchI\n",
            "(3770, 1024)\n",
            "(3879, 1024)\n",
            "jcoYJXDG9sw\n",
            "(5940, 1024)\n",
            "(5954, 1024)\n",
            "xmEERLqJ2kU\n",
            "(12934, 1024)\n",
            "(13348, 1024)\n",
            "xxdtq8mxegs\n",
            "(4176, 1024)\n",
            "(4307, 1024)\n",
            "vdmoEJ5YbrQ\n",
            "(9840, 1024)\n",
            "(9853, 1024)\n",
            "xwqBXPGE9pQ\n",
            "(6960, 1024)\n",
            "(6993, 1024)\n",
            "z_6gVvQb2d0\n",
            "(8004, 1024)\n",
            "(8264, 1024)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "flow_path = '/content/drive/MyDrive/MSVA/datasets/kinetic_features/tvsum/FLOW/features'\n",
        "for file_name in os.listdir('./'):\n",
        "\n",
        "  now_df = pd.read_csv(file_name)\n",
        "  file_name = file_name[:len(file_name) - 7]\n",
        "  print(file_name)\n",
        "  video_sec = len(now_df) * 2\n",
        "  now = np.load(flow_path + os.sep +file_name + \".npy\")\n",
        "  a = now.shape\n",
        "  frame_cnt = a[0] // video_sec\n",
        "  res = np.empty((0,1024), int)\n",
        "  for i in range(len(now_df)):\n",
        "    if now_df['Predicted Labels'][i] == '중립':\n",
        "      for _ in range(frame_cnt * 2):\n",
        "        res = np.append(res, np.array([np.zeros(1024)]), axis=0)\n",
        "    else:\n",
        "      for _ in range(frame_cnt * 2):\n",
        "        res = np.append(res, np.array([np.ones(1024)]), axis=0)\n",
        "\n",
        "  print(res.shape)\n",
        "  print(a)\n",
        "  np.save(os.path.join('./', file_name+\".npy\"), res)\n",
        "  # print(file_name, b.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzCZfTR8awal",
        "outputId": "a3719b23-d413-4d75-dae3-8fc875c57c8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2021.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "7!pip install pandas74"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv6ywGkDayfi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "MSVA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}